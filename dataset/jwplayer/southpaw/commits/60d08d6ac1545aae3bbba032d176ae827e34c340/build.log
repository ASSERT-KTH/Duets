[INFO] Error stacktraces are turned on.
[INFO] Scanning for projects...
[INFO] 
[INFO] -----------------------< com.jwplayer:southpaw >------------------------
[INFO] Building southpaw 0.5.1
[INFO] --------------------------------[ jar ]---------------------------------
[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.5:prepare-agent (default) @ southpaw ---
[INFO] argLine set to -javaagent:/home/jdbl/.m2/repository/org/jacoco/org.jacoco.agent/0.8.5/org.jacoco.agent-0.8.5-runtime.jar=destfile=/tmp/tmph9yrc09t/southpaw/target/jacoco.exec
[INFO] 
[INFO] --- jsonschema2pojo-maven-plugin:1.0.0:generate (default) @ southpaw ---
[INFO] 
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @ southpaw ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] Copying 3 resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:compile (default-compile) @ southpaw ---
[INFO] Changes detected - recompiling the module!
[WARNING] File encoding has not been set, using platform encoding UTF-8, i.e. build is platform dependent!
[INFO] Compiling 31 source files to /tmp/tmph9yrc09t/southpaw/target/classes
[WARNING] /tmp/tmph9yrc09t/southpaw/src/main/java/com/jwplayer/southpaw/Southpaw.java: /tmp/tmph9yrc09t/southpaw/src/main/java/com/jwplayer/southpaw/Southpaw.java uses unchecked or unsafe operations.
[WARNING] /tmp/tmph9yrc09t/southpaw/src/main/java/com/jwplayer/southpaw/Southpaw.java: Recompile with -Xlint:unchecked for details.
[INFO] 
[INFO] --- maven-resources-plugin:2.6:testResources (default-testResources) @ southpaw ---
[WARNING] Using platform encoding (UTF-8 actually) to copy filtered resources, i.e. build is platform dependent!
[INFO] skip non existing resourceDirectory /tmp/tmph9yrc09t/southpaw/src/test/resources
[INFO] 
[INFO] --- maven-compiler-plugin:3.1:testCompile (default-testCompile) @ southpaw ---
[INFO] Changes detected - recompiling the module!
[WARNING] File encoding has not been set, using platform encoding UTF-8, i.e. build is platform dependent!
[INFO] Compiling 24 source files to /tmp/tmph9yrc09t/southpaw/target/test-classes
[INFO] 
[INFO] --- maven-surefire-plugin:2.19.1:test (default-test) @ southpaw ---
[INFO] Surefire report directory: /tmp/tmph9yrc09t/southpaw/target/surefire-reports

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.jwplayer.southpaw.record.MapRecordTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.087 sec - in com.jwplayer.southpaw.record.MapRecordTest
Running com.jwplayer.southpaw.record.AvroRecordTest
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.431 sec - in com.jwplayer.southpaw.record.AvroRecordTest
Running com.jwplayer.southpaw.index.MultiIndexTest
23:05:12,779 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:13,130 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:13,131 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:13,132 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:13,519 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:13,520 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:14,079 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:14,080 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:14,080 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:14,081 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:15,625 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:15,626 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:16,036 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:16,036 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:16,037 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:16,037 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:16,181 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:16,181 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:16,256 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:16,257 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:16,257 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:16,257 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:16,441 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:16,445 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:16,876 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:16,877 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:16,877 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:16,877 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:17,304 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:17,305 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:17,380 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:17,381 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:17,381 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:17,381 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

Verifying reverse index: TestIndex-reverse against index: TestIndex
23:05:17,523 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:17,524 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:17,585 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:17,586 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:17,586 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:17,586 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:17,758 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:17,759 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:17,820 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:17,820 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:17,820 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:17,821 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:17,907 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:17,915 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:17,985 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:17,986 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:17,986 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:17,986 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:18,077 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:18,078 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:18,145 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:18,146 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:18,146 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:18,146 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:18,322 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:18,323 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:18,385 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:18,385 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:18,385 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:18,386 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:18,477 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:18,477 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:18,477 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:18,478 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:18,517 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:18,518 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:18,578 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:18,579 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:18,579 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:18,580 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:18,663 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:18,663 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:18,663 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:18,663 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:18,699 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:18,700 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:18,776 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:18,776 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:18,777 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:18,778 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:18,950 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:18,951 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:19,006 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:19,006 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:19,006 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:19,007 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:19,173 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:19,174 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:19,231 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:19,231 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:19,232 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:19,232 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:19,315 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:19,316 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:19,372 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:19,372 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:19,373 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:19,373 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

Verifying index: TestIndex against reverse index: TestIndex-reverse
23:05:19,537 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:19,541 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:19,598 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:19,598 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:19,598 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:19,598 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:19,697 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:19,697 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:19,751 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:19,752 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:19,752 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:19,752 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:21,091 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:21,092 INFO  state.RocksDBState             - RocksDB state has been deleted
Tests run: 18, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.402 sec - in com.jwplayer.southpaw.index.MultiIndexTest
Running com.jwplayer.southpaw.SouthpawTest
23:05:21,318 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:21,319 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:21,319 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:21,319 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:21,352 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:21,352 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:21,352 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:21,353 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:21,484 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:21,485 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:21,485 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:21,485 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:21,523 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:21,524 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:21,524 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:21,524 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:21,589 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:21,589 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:21,589 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:21,589 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:21,727 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:21,728 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:21,728 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:21,728 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:22,142 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:22,142 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:22,142 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:22,143 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:22,445 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:22,446 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:22,446 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:22,446 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,535 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:23,544 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:23,546 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:23,547 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:23,547 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:23,548 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:23,649 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,649 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,650 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,650 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,685 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,685 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,685 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,686 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,719 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,719 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,720 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,720 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,756 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,757 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,757 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,757 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,804 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,804 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,804 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,805 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,841 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,841 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,841 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,841 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,880 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,881 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,881 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,881 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,927 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,927 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:23,927 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:23,927 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,506 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:24,507 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:24,510 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:24,510 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:24,517 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:24,525 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:24,691 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,691 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,691 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,691 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,724 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,724 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,725 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,725 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,762 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,762 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,762 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,763 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,808 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,809 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,813 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,814 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,851 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,851 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,851 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,852 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,887 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,887 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,887 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,888 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,935 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,935 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,936 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,936 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,987 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,987 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:24,987 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:24,987 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,581 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:25,581 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:25,583 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:25,584 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:25,584 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:25,585 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:25,699 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,700 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,700 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,700 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,734 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,734 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,734 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,734 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,769 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,770 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,770 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,770 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,805 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,805 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,806 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,806 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,839 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,840 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,840 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,840 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,875 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,875 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,875 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,877 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,916 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,925 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:25,953 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:25,954 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:26,003 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:26,006 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:26,007 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:26,007 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:26,650 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:26,650 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:26,652 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:26,653 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:26,654 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:26,657 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:27,136 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,136 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,136 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,136 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,169 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,170 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,171 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,172 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,213 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,213 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,215 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,215 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,253 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,254 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,254 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,255 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,292 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,292 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,292 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,292 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,329 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,329 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,329 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,330 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,368 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,368 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,369 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,369 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,407 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,409 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,409 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:27,410 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:27,974 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:27,974 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:27,976 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:27,976 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:27,976 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:27,977 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:28,095 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,096 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,097 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,097 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,130 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,131 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,131 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,131 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,175 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,176 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,176 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,176 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,212 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,212 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,212 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,212 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,261 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,262 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,263 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,263 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,304 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,304 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,304 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,304 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,344 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,344 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,344 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,344 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,388 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,389 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,389 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:28,389 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:28,996 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:28,996 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:28,998 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:28,999 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:28,999 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:29,000 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:29,158 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,158 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,158 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,158 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,194 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,194 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,194 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,194 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,229 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,229 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,229 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,229 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,265 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,266 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,267 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,267 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,302 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,303 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,303 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,303 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,342 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,343 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,343 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,343 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,391 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,392 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,392 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,392 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,434 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,434 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:29,435 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:29,435 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,048 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:30,049 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:30,056 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:30,059 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:30,060 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:30,061 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:30,186 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,187 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,187 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,188 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,221 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,221 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,221 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,221 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,264 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,264 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,264 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,265 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,297 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,298 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,298 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,298 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,334 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,335 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,335 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,335 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,375 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,375 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,375 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,375 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,416 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,417 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,417 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,418 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,459 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,460 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:30,460 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:30,461 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,074 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:31,074 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:31,076 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:31,076 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:31,076 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:31,081 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:31,181 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,181 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,188 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,188 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,228 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,228 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,228 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,228 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,260 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,260 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,260 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,260 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,293 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,293 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,293 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,293 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,331 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,331 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,331 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,332 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,362 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,362 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,362 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,362 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,408 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,408 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,408 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,409 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,447 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,448 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:31,448 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:31,449 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:32,043 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:32,043 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:32,045 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:32,045 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:32,045 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:32,046 INFO  state.RocksDBState             - RocksDB state has been deleted
Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.953 sec - in com.jwplayer.southpaw.SouthpawTest
Running com.jwplayer.southpaw.state.BaseStateTest
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in com.jwplayer.southpaw.state.BaseStateTest
Running com.jwplayer.southpaw.state.RocksDBStateTest
23:05:33,124 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:33,128 WARN  state.RocksDBState             - Skipping state restore, no backups found in backup URI
23:05:33,142 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:33,251 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:33,251 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:33,349 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:33,350 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:33,350 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:33,350 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:33,461 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:33,547 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:33,553 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:33,556 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:33,557 WARN  state.RocksDBState             - Skipping state restore, no backups found in backup URI
23:05:33,561 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:33,670 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:33,671 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:33,671 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:33,672 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:33,776 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:33,779 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:33,780 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:33,789 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:33,789 INFO  state.RocksDBState             - Attempting to restore latest backup
23:05:33,797 INFO  state.RocksDBState             - Backup restored
23:05:33,805 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:33,848 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:33,849 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:33,854 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:33,854 WARN  state.RocksDBState             - Skipping state restore, no backups found in backup URI
23:05:33,865 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:33,951 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:33,951 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:33,964 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:33,965 WARN  state.RocksDBState             - Skipping state restore, no backups found in backup URI
23:05:33,973 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:34,168 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:34,168 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:34,272 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:34,277 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:34,277 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:34,277 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:34,378 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:34,385 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:34,580 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:34,581 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:34,689 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:34,690 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:34,797 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:34,798 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:34,798 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:34,798 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:34,885 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:34,885 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:34,886 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:34,893 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:34,893 INFO  state.RocksDBState             - Attempting to restore latest backup
23:05:34,905 INFO  state.RocksDBState             - Backup restored
23:05:34,913 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:34,966 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:34,966 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:34,967 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:34,977 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:34,980 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:34,981 WARN  state.RocksDBState             - Skipping state restore, no backups found in backup URI
23:05:34,989 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:35,142 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:35,142 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:35,142 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:35,143 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:35,230 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:35,232 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:35,237 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:35,239 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:35,240 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:35,241 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:35,242 INFO  state.RocksDBState             - Attempting to restore latest backup
23:05:35,246 INFO  state.RocksDBState             - Backup restored
23:05:35,250 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:35,297 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:35,301 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:35,307 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:35,307 WARN  state.RocksDBState             - Skipping state restore, no backups found in backup URI
23:05:35,313 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:35,458 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:35,458 WARN  state.RocksDBState             - Skipping state restore, no backups found in backup URI
23:05:35,465 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:35,677 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:35,678 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:35,941 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:35,942 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:35,942 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:35,942 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:36,251 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:36,252 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:36,253 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:36,257 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:36,258 INFO  state.RocksDBState             - Attempting to restore backup 1 of 1
23:05:36,260 INFO  state.RocksDBState             - Backup restored
23:05:36,271 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:36,346 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:36,347 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:36,347 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:36,353 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:36,356 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:36,356 WARN  state.RocksDBState             - Skipping state restore, no backups found in backup URI
23:05:36,365 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:36,459 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:36,459 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:36,459 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:36,460 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:36,557 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:36,559 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:36,559 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:36,565 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:36,565 INFO  state.RocksDBState             - Attempting to restore backup 1 of 1
23:05:36,569 INFO  state.RocksDBState             - Backup restored
23:05:36,577 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:36,740 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:36,741 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:36,742 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:36,757 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:36,865 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:36,866 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:36,866 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:36,866 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:36,969 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:36,969 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:36,970 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:37,045 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:37,046 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:37,046 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:37,141 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:37,149 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:37,150 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:37,157 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:37,157 INFO  state.RocksDBState             - Attempting to restore backup 1 of 3
23:05:37,161 WARN  state.RocksDBState             - Failed to restore backup 1 of 3 with exception: Checksum check failed
23:05:37,162 INFO  state.RocksDBState             - Attempting to restore backup 2 of 3
23:05:37,169 INFO  state.RocksDBState             - Backup restored
23:05:37,190 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:37,234 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:37,234 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:37,356 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:37,356 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:37,357 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:37,365 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:37,365 INFO  state.RocksDBState             - Attempting to restore backup 1 of 3
23:05:37,366 INFO  state.RocksDBState             - Backup restored
23:05:37,373 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:37,441 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:37,443 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:37,443 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:37,449 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:37,556 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:37,561 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:37,691 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:37,693 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:37,825 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:37,829 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:37,956 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:37,957 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:38,035 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:38,035 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:38,035 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:38,035 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:38,117 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:38,117 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:38,118 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:38,121 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:38,121 INFO  state.RocksDBState             - Attempting to restore latest backup
23:05:38,129 INFO  state.RocksDBState             - Backup restored
23:05:38,137 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:38,288 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:38,288 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:38,289 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:38,293 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:38,385 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:38,386 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:38,388 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:38,408 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:38,409 WARN  state.RocksDBState             - Skipping state restore, no backups found in backup URI
23:05:38,417 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:38,505 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:38,505 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:38,505 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:38,505 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:38,616 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:38,694 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:38,697 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:38,847 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:38,848 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:38,848 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:38,848 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:38,953 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:38,954 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:38,956 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:39,066 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:39,067 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:39,071 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:39,071 WARN  state.RocksDBState             - Skipping state restore, no backups found in backup URI
23:05:39,073 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:39,177 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:39,178 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:39,178 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:39,178 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:39,305 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:39,306 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:39,313 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:39,314 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:39,315 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:39,316 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:39,316 INFO  state.RocksDBState             - Attempting to restore latest backup
23:05:39,329 INFO  state.RocksDBState             - Backup restored
23:05:39,333 INFO  state.RocksDBState             - RocksDB state restoration complete
23:05:39,377 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:39,379 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:39,533 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:39,533 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:39,534 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:39,534 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:39,619 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:39,620 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:39,620 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:39,621 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:39,622 INFO  state.RocksDBState             - Attempting to restore backup 1 of 1
23:05:39,627 WARN  state.RocksDBState             - Failed to restore backup 1 of 1 with exception: Checksum check failed
23:05:39,627 ERROR state.RocksDBState             - Failed to restore all backups
23:05:39,633 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:39,641 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:39,750 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:39,753 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:39,934 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:39,934 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:40,046 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:40,047 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:40,047 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:40,048 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:40,144 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:40,145 INFO  state.RocksDBState             - Restoring RocksDB state from backups
23:05:40,145 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:40,150 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:40,150 INFO  state.RocksDBState             - Attempting to restore latest backup
23:05:40,157 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:40,157 INFO  state.RocksDBState             - RocksDB state has been deleted
23:05:40,284 INFO  state.RocksDBState             - Deleting RocksDB state
23:05:40,285 INFO  state.RocksDBState             - RocksDB state has been deleted
Tests run: 29, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 8.233 sec - in com.jwplayer.southpaw.state.RocksDBStateTest
Running com.jwplayer.southpaw.util.FileHelperTest
Tests run: 3, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in com.jwplayer.southpaw.util.FileHelperTest
Running com.jwplayer.southpaw.util.S3HelperTest
23:05:44,554 INFO  s3mock.S3Mock                  - bound to 0.0.0.0:8001
log4j:WARN No appenders could be found for logger (com.amazonaws.AmazonWebServiceClient).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
23:05:45,674 INFO  route.CreateBucket             - PUT bucket bucket.com
23:05:45,996 INFO  route.PutObject                - put object bucket.com/some/path/fileA.txt (unsigned)
23:05:46,121 INFO  route.PutObject                - put object bucket.com/some/path/fileB.txt (unsigned)
23:05:46,154 INFO  route.PutObject                - put object bucket.com/some/path/fileC.txt (unsigned)
23:05:46,589 INFO  s3mock.S3Mock                  - bound to 0.0.0.0:8001
23:05:46,668 INFO  route.CreateBucket             - PUT bucket bucket.com
23:05:46,725 INFO  route.PutObject                - put object bucket.com/some/path/fileA.txt (unsigned)
23:05:46,773 INFO  route.PutObject                - put object bucket.com/some/path/fileB.txt (unsigned)
23:05:46,796 INFO  route.PutObject                - put object bucket.com/some/path/fileC.txt (unsigned)
23:05:47,169 INFO  s3mock.S3Mock                  - bound to 0.0.0.0:8001
23:05:47,208 INFO  route.CreateBucket             - PUT bucket bucket.com
23:05:47,218 INFO  route.PutObject                - put object bucket.com/some/path/fileA.txt (unsigned)
23:05:47,250 INFO  route.PutObject                - put object bucket.com/some/path/fileB.txt (unsigned)
23:05:47,280 INFO  route.PutObject                - put object bucket.com/some/path/fileC.txt (unsigned)
23:05:47,367 INFO  route.ListBucket               - listing bucket bucket.com with prefix=Some(some/path), delimiter=None
23:05:47,773 INFO  s3mock.S3Mock                  - bound to 0.0.0.0:8001
23:05:47,809 INFO  route.CreateBucket             - PUT bucket bucket.com
23:05:47,815 INFO  route.PutObject                - put object bucket.com/some/path/fileA.txt (unsigned)
23:05:47,829 INFO  route.PutObject                - put object bucket.com/some/path/fileB.txt (unsigned)
23:05:47,852 INFO  route.PutObject                - put object bucket.com/some/path/fileC.txt (unsigned)
23:05:47,878 INFO  util.S3Helper                  - Initiating sync from S3
23:05:47,883 INFO  route.ListBucket               - listing bucket bucket.com with prefix=Some(some/path), delimiter=None
23:05:47,910 INFO  util.S3Helper                  - Downloading files from s3://bucket.com/some/path to file:/tmp/junit1697464578354770499/
23:05:48,326 INFO  util.S3Helper                  - Downloaded 3 files
23:05:48,326 INFO  util.S3Helper                  - Sync from S3 complete
23:05:48,558 INFO  s3mock.S3Mock                  - bound to 0.0.0.0:8001
23:05:48,580 INFO  route.CreateBucket             - PUT bucket bucket.com
23:05:48,607 INFO  route.PutObject                - put object bucket.com/some/path/fileA.txt (unsigned)
23:05:48,627 INFO  route.PutObject                - put object bucket.com/some/path/fileB.txt (unsigned)
23:05:48,643 INFO  route.PutObject                - put object bucket.com/some/path/fileC.txt (unsigned)
23:05:48,688 INFO  route.ListBucket               - listing bucket bucket.com with prefix=Some(some/path), delimiter=None
23:05:48,762 INFO  route.DeleteObjects            - deleted object bucket.com/some/path/fileA.txt
23:05:48,762 INFO  route.DeleteObjects            - deleted object bucket.com/some/path/fileB.txt
23:05:48,762 INFO  route.DeleteObjects            - deleted object bucket.com/some/path/fileC.txt
23:05:48,778 INFO  route.ListBucket               - listing bucket bucket.com with prefix=Some(some/path), delimiter=None
23:05:48,949 INFO  s3mock.S3Mock                  - bound to 0.0.0.0:8001
23:05:48,979 INFO  route.CreateBucket             - PUT bucket bucket.com
23:05:49,004 INFO  route.PutObject                - put object bucket.com/some/path/fileA.txt (unsigned)
23:05:49,039 INFO  route.PutObject                - put object bucket.com/some/path/fileB.txt (unsigned)
23:05:49,056 INFO  route.PutObject                - put object bucket.com/some/path/fileC.txt (unsigned)
23:05:49,075 INFO  util.S3Helper                  - Initiating background sync to S3
23:05:49,081 INFO  util.S3Helper                  - Blocking for existing sync to S3 to complete
23:05:49,108 INFO  route.ListBucket               - listing bucket bucket.com with prefix=Some(backups), delimiter=None
23:05:49,130 INFO  util.S3Helper                  - Uploading files from file:/tmp/junit4244572070278674558/ to s3://bucket.com/backups
23:05:49,265 INFO  route.PutObject                - put object bucket.com/backups/media.txt (unsigned)
23:05:49,284 INFO  route.PutObject                - put object bucket.com/backups/feed.txt (unsigned)
23:05:49,284 INFO  route.PutObject                - put object bucket.com/backups/account.txt (unsigned)
23:05:49,275 WARN  route.PutObject                - Unable to parse last modified date: Thu Jul 30 23:05:49 UTC 2020
java.lang.IllegalArgumentException: Invalid format: "Thu Jul 30 23:05:49 UTC 2020" is malformed at " Jul 30 23:05:49 UTC 2020"
	at org.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:187) ~[joda-time-2.8.1.jar:2.8.1]
	at org.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780) ~[joda-time-2.8.1.jar:2.8.1]
	at com.amazonaws.util.DateUtils.parseRFC822Date(DateUtils.java:196) ~[aws-java-sdk-core-1.11.292.jar:?]
	at com.amazonaws.services.s3.internal.ServiceUtils.parseRfc822Date(ServiceUtils.java:87) ~[aws-java-sdk-s3-1.11.292.jar:?]
	at io.findify.s3mock.route.PutObject.$anonfun$populateObjectMetadata$1(PutObject.scala:118) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at io.findify.s3mock.route.PutObject.$anonfun$populateObjectMetadata$1$adapted(PutObject.scala:108) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at scala.collection.Iterator.foreach(Iterator.scala:929) ~[scala-library-2.12.3.jar:?]
	at scala.collection.Iterator.foreach$(Iterator.scala:929) ~[scala-library-2.12.3.jar:?]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417) ~[scala-library-2.12.3.jar:?]
	at scala.collection.IterableLike.foreach(IterableLike.scala:71) ~[scala-library-2.12.3.jar:?]
	at scala.collection.IterableLike.foreach$(IterableLike.scala:70) ~[scala-library-2.12.3.jar:?]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[scala-library-2.12.3.jar:?]
	at io.findify.s3mock.route.PutObject.populateObjectMetadata(PutObject.scala:108) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at io.findify.s3mock.route.PutObject.$anonfun$completePlain$4(PutObject.scala:78) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at akka.stream.impl.fusing.Map$$anon$9.onPush(Ops.scala:53) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:519) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreter.processEvent(GraphInterpreter.scala:508) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:378) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:585) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreterShell$AsyncInput.execute(ActorGraphInterpreter.scala:469) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreterShell.processEvent(ActorGraphInterpreter.scala:560) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.ActorGraphInterpreter.akka$stream$impl$fusing$ActorGraphInterpreter$$processEvent(ActorGraphInterpreter.scala:742) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.ActorGraphInterpreter$$anonfun$receive$1.applyOrElse(ActorGraphInterpreter.scala:757) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.actor.Actor.aroundReceive(Actor.scala:517) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.actor.Actor.aroundReceive$(Actor.scala:515) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundReceive(ActorGraphInterpreter.scala:667) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:590) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.actor.ActorCell.invoke(ActorCell.scala:559) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.Mailbox.run(Mailbox.scala:224) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.12-2.5.11.jar:2.5.11]
23:05:49,295 WARN  route.PutObject                - Unable to parse last modified date: Thu Jul 30 23:05:49 UTC 2020
java.lang.IllegalArgumentException: Invalid format: "Thu Jul 30 23:05:49 UTC 2020" is malformed at " Jul 30 23:05:49 UTC 2020"
	at org.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:187) ~[joda-time-2.8.1.jar:2.8.1]
	at org.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780) ~[joda-time-2.8.1.jar:2.8.1]
	at com.amazonaws.util.DateUtils.parseRFC822Date(DateUtils.java:196) ~[aws-java-sdk-core-1.11.292.jar:?]
	at com.amazonaws.services.s3.internal.ServiceUtils.parseRfc822Date(ServiceUtils.java:87) ~[aws-java-sdk-s3-1.11.292.jar:?]
	at io.findify.s3mock.route.PutObject.$anonfun$populateObjectMetadata$1(PutObject.scala:118) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at io.findify.s3mock.route.PutObject.$anonfun$populateObjectMetadata$1$adapted(PutObject.scala:108) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at scala.collection.Iterator.foreach(Iterator.scala:929) ~[scala-library-2.12.3.jar:?]
	at scala.collection.Iterator.foreach$(Iterator.scala:929) ~[scala-library-2.12.3.jar:?]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417) ~[scala-library-2.12.3.jar:?]
	at scala.collection.IterableLike.foreach(IterableLike.scala:71) ~[scala-library-2.12.3.jar:?]
	at scala.collection.IterableLike.foreach$(IterableLike.scala:70) ~[scala-library-2.12.3.jar:?]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[scala-library-2.12.3.jar:?]
	at io.findify.s3mock.route.PutObject.populateObjectMetadata(PutObject.scala:108) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at io.findify.s3mock.route.PutObject.$anonfun$completePlain$4(PutObject.scala:78) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at akka.stream.impl.fusing.Map$$anon$9.onPush(Ops.scala:53) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:519) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreter.processEvent(GraphInterpreter.scala:508) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:378) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:585) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreterShell$AsyncInput.execute(ActorGraphInterpreter.scala:469) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreterShell.processEvent(ActorGraphInterpreter.scala:560) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.ActorGraphInterpreter.akka$stream$impl$fusing$ActorGraphInterpreter$$processEvent(ActorGraphInterpreter.scala:742) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.ActorGraphInterpreter$$anonfun$receive$1.applyOrElse(ActorGraphInterpreter.scala:757) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.actor.Actor.aroundReceive(Actor.scala:517) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.actor.Actor.aroundReceive$(Actor.scala:515) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundReceive(ActorGraphInterpreter.scala:667) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:590) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.actor.ActorCell.invoke(ActorCell.scala:559) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.Mailbox.run(Mailbox.scala:224) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.12-2.5.11.jar:2.5.11]
23:05:49,295 WARN  route.PutObject                - Unable to parse last modified date: Thu Jul 30 23:05:49 UTC 2020
java.lang.IllegalArgumentException: Invalid format: "Thu Jul 30 23:05:49 UTC 2020" is malformed at " Jul 30 23:05:49 UTC 2020"
	at org.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:187) ~[joda-time-2.8.1.jar:2.8.1]
	at org.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780) ~[joda-time-2.8.1.jar:2.8.1]
	at com.amazonaws.util.DateUtils.parseRFC822Date(DateUtils.java:196) ~[aws-java-sdk-core-1.11.292.jar:?]
	at com.amazonaws.services.s3.internal.ServiceUtils.parseRfc822Date(ServiceUtils.java:87) ~[aws-java-sdk-s3-1.11.292.jar:?]
	at io.findify.s3mock.route.PutObject.$anonfun$populateObjectMetadata$1(PutObject.scala:118) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at io.findify.s3mock.route.PutObject.$anonfun$populateObjectMetadata$1$adapted(PutObject.scala:108) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at scala.collection.Iterator.foreach(Iterator.scala:929) ~[scala-library-2.12.3.jar:?]
	at scala.collection.Iterator.foreach$(Iterator.scala:929) ~[scala-library-2.12.3.jar:?]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417) ~[scala-library-2.12.3.jar:?]
	at scala.collection.IterableLike.foreach(IterableLike.scala:71) ~[scala-library-2.12.3.jar:?]
	at scala.collection.IterableLike.foreach$(IterableLike.scala:70) ~[scala-library-2.12.3.jar:?]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[scala-library-2.12.3.jar:?]
	at io.findify.s3mock.route.PutObject.populateObjectMetadata(PutObject.scala:108) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at io.findify.s3mock.route.PutObject.$anonfun$completePlain$4(PutObject.scala:78) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at akka.stream.impl.fusing.Map$$anon$9.onPush(Ops.scala:53) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:519) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreter.processEvent(GraphInterpreter.scala:508) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:378) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:585) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreterShell$AsyncInput.execute(ActorGraphInterpreter.scala:469) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreterShell.processEvent(ActorGraphInterpreter.scala:560) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.ActorGraphInterpreter.akka$stream$impl$fusing$ActorGraphInterpreter$$processEvent(ActorGraphInterpreter.scala:742) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.ActorGraphInterpreter$$anonfun$receive$1.applyOrElse(ActorGraphInterpreter.scala:757) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.actor.Actor.aroundReceive(Actor.scala:517) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.actor.Actor.aroundReceive$(Actor.scala:515) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundReceive(ActorGraphInterpreter.scala:667) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:590) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.actor.ActorCell.invoke(ActorCell.scala:559) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.Mailbox.run(Mailbox.scala:224) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.12-2.5.11.jar:2.5.11]
23:05:49,337 INFO  util.S3Helper                  - Uploaded 3 files
23:05:49,337 INFO  util.S3Helper                  - Background sync to S3 complete
23:05:49,338 INFO  util.S3Helper                  - Existing sync to S3 complete
23:05:49,338 INFO  util.S3Helper                  - Initiating background sync to S3
23:05:49,338 INFO  util.S3Helper                  - Blocking for existing sync to S3 to complete
23:05:49,367 INFO  route.ListBucket               - listing bucket bucket.com with prefix=Some(backups), delimiter=None
23:05:49,377 INFO  util.S3Helper                  - Uploading files from file:/tmp/junit4244572070278674558/ to s3://bucket.com/backups
23:05:49,419 INFO  route.PutObject                - put object bucket.com/backups/player.txt (unsigned)
23:05:49,430 WARN  route.PutObject                - Unable to parse last modified date: Thu Jul 30 23:05:49 UTC 2020
java.lang.IllegalArgumentException: Invalid format: "Thu Jul 30 23:05:49 UTC 2020" is malformed at " Jul 30 23:05:49 UTC 2020"
	at org.joda.time.format.DateTimeParserBucket.doParseMillis(DateTimeParserBucket.java:187) ~[joda-time-2.8.1.jar:2.8.1]
	at org.joda.time.format.DateTimeFormatter.parseMillis(DateTimeFormatter.java:780) ~[joda-time-2.8.1.jar:2.8.1]
	at com.amazonaws.util.DateUtils.parseRFC822Date(DateUtils.java:196) ~[aws-java-sdk-core-1.11.292.jar:?]
	at com.amazonaws.services.s3.internal.ServiceUtils.parseRfc822Date(ServiceUtils.java:87) ~[aws-java-sdk-s3-1.11.292.jar:?]
	at io.findify.s3mock.route.PutObject.$anonfun$populateObjectMetadata$1(PutObject.scala:118) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at io.findify.s3mock.route.PutObject.$anonfun$populateObjectMetadata$1$adapted(PutObject.scala:108) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at scala.collection.Iterator.foreach(Iterator.scala:929) ~[scala-library-2.12.3.jar:?]
	at scala.collection.Iterator.foreach$(Iterator.scala:929) ~[scala-library-2.12.3.jar:?]
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1417) ~[scala-library-2.12.3.jar:?]
	at scala.collection.IterableLike.foreach(IterableLike.scala:71) ~[scala-library-2.12.3.jar:?]
	at scala.collection.IterableLike.foreach$(IterableLike.scala:70) ~[scala-library-2.12.3.jar:?]
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54) ~[scala-library-2.12.3.jar:?]
	at io.findify.s3mock.route.PutObject.populateObjectMetadata(PutObject.scala:108) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at io.findify.s3mock.route.PutObject.$anonfun$completePlain$4(PutObject.scala:78) ~[s3mock_2.12-0.2.5.jar:0.2.5]
	at akka.stream.impl.fusing.Map$$anon$9.onPush(Ops.scala:53) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreter.processPush(GraphInterpreter.scala:519) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreter.processEvent(GraphInterpreter.scala:508) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreter.execute(GraphInterpreter.scala:378) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreterShell.runBatch(ActorGraphInterpreter.scala:585) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreterShell$AsyncInput.execute(ActorGraphInterpreter.scala:469) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.GraphInterpreterShell.processEvent(ActorGraphInterpreter.scala:560) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.ActorGraphInterpreter.akka$stream$impl$fusing$ActorGraphInterpreter$$processEvent(ActorGraphInterpreter.scala:742) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.ActorGraphInterpreter$$anonfun$receive$1.applyOrElse(ActorGraphInterpreter.scala:757) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.actor.Actor.aroundReceive(Actor.scala:517) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.actor.Actor.aroundReceive$(Actor.scala:515) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.stream.impl.fusing.ActorGraphInterpreter.aroundReceive(ActorGraphInterpreter.scala:667) [akka-stream_2.12-2.5.11.jar:2.5.11]
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:590) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.actor.ActorCell.invoke(ActorCell.scala:559) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.Mailbox.run(Mailbox.scala:224) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.Mailbox.exec(Mailbox.scala:234) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [akka-actor_2.12-2.5.11.jar:2.5.11]
	at akka.dispatch.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [akka-actor_2.12-2.5.11.jar:2.5.11]
23:05:49,441 INFO  util.S3Helper                  - Uploaded 1 files
23:05:49,460 INFO  route.DeleteObjects            - deleted object bucket.com/backups/feed.txt
23:05:49,463 INFO  util.S3Helper                  - Deleted 1 files
23:05:49,463 INFO  util.S3Helper                  - Background sync to S3 complete
23:05:49,463 INFO  util.S3Helper                  - Existing sync to S3 complete
23:05:49,466 INFO  route.ListBucket               - listing bucket bucket.com with prefix=Some(backups), delimiter=None
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 9.281 sec - in com.jwplayer.southpaw.util.S3HelperTest
Running com.jwplayer.southpaw.util.ByteArraySetTest
Tests run: 17, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.315 sec - in com.jwplayer.southpaw.util.ByteArraySetTest
Running com.jwplayer.southpaw.util.ByteArrayTest
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.007 sec - in com.jwplayer.southpaw.util.ByteArrayTest
Running com.jwplayer.southpaw.serde.AvroSerdeTest
23:05:49,988 INFO  serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	specific.avro.reader = false

23:05:49,989 INFO  serializers.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000

23:05:49,990 INFO  serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	specific.avro.reader = false

23:05:49,990 INFO  serializers.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000

23:05:50,026 INFO  serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	specific.avro.reader = false

23:05:50,026 INFO  serializers.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000

23:05:50,027 INFO  serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	specific.avro.reader = false

23:05:50,027 INFO  serializers.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000

23:05:50,027 INFO  serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	specific.avro.reader = false

23:05:50,027 INFO  serializers.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000

23:05:50,059 INFO  serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	specific.avro.reader = false

23:05:50,059 INFO  serializers.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000

23:05:50,060 INFO  serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000
	specific.avro.reader = false

23:05:50,060 INFO  serializers.KafkaAvroSerializerConfig - KafkaAvroSerializerConfig values: 
	schema.registry.url = [nowhere]
	auto.register.schemas = true
	max.schemas.per.subject = 1000

Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.145 sec - in com.jwplayer.southpaw.serde.AvroSerdeTest
Running com.jwplayer.southpaw.serde.JacksonSerdeTest
Tests run: 5, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.053 sec - in com.jwplayer.southpaw.serde.JacksonSerdeTest
Running com.jwplayer.southpaw.serde.JsonSerdeTest
23:05:50,117 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,117 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,145 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,145 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,147 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,147 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,149 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,156 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,158 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,158 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,161 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,161 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,162 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,162 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,163 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,163 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,165 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,165 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

Tests run: 9, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.051 sec - in com.jwplayer.southpaw.serde.JsonSerdeTest
Running com.jwplayer.southpaw.SouthpawEndToEndTest
23:05:50,302 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,302 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,302 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,302 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,337 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,338 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,338 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,338 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,372 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,372 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,372 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,373 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,407 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,407 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,407 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,408 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,450 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,451 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,451 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,451 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,496 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,496 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,497 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,497 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,532 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,532 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,532 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,532 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,569 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,569 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,569 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,569 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,612 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,613 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,613 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,613 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,659 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,659 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,659 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,660 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,660 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,660 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,660 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,660 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,717 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,717 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,717 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,717 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,718 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,718 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,718 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,718 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,718 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,718 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,718 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,718 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,718 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,718 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,719 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,719 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,719 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,719 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:50,719 INFO  serializers.KafkaJsonDeserializerConfig - KafkaJsonDeserializerConfig values: 
	json.fail.unknown.properties = true
	json.key.type = class java.lang.Object
	json.value.type = class java.lang.Object

23:05:50,719 INFO  serializers.KafkaJsonSerializerConfig - KafkaJsonSerializerConfig values: 
	json.indent.output = false

23:05:51,845 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:05:51,845 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:05:51,890 INFO  southpaw.Southpaw              - Building denormalized records
23:05:52,892 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:05:54,099 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:54,099 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:54,260 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:54,261 INFO  southpaw.Southpaw              - Building denormalized records
23:05:55,262 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:05:57,489 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:57,489 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:57,652 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:57,654 INFO  southpaw.Southpaw              - Building denormalized records
23:05:58,655 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:05:59,848 INFO  state.RocksDBState             - Backing up RocksDB state
23:05:59,848 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:05:59,987 INFO  state.RocksDBState             - RocksDB state backup complete
23:05:59,989 INFO  southpaw.Southpaw              - Building denormalized records
23:06:00,990 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:06:02,564 INFO  state.RocksDBState             - Backing up RocksDB state
23:06:02,565 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:06:02,731 INFO  state.RocksDBState             - RocksDB state backup complete
23:06:02,733 INFO  southpaw.Southpaw              - Building denormalized records
23:06:03,734 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:06:07,478 INFO  state.RocksDBState             - Backing up RocksDB state
23:06:07,478 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:06:07,601 INFO  state.RocksDBState             - RocksDB state backup complete
23:06:07,602 INFO  southpaw.Southpaw              - Building denormalized records
23:06:08,604 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:06:09,603 INFO  state.RocksDBState             - Backing up RocksDB state
23:06:09,603 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:06:09,755 INFO  state.RocksDBState             - RocksDB state backup complete
23:06:09,757 INFO  southpaw.Southpaw              - Building denormalized records
23:06:10,758 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:06:13,589 INFO  state.RocksDBState             - Backing up RocksDB state
23:06:13,589 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:06:13,726 INFO  state.RocksDBState             - RocksDB state backup complete
23:06:13,727 INFO  southpaw.Southpaw              - Building denormalized records
23:06:14,728 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:06:15,666 INFO  state.RocksDBState             - Backing up RocksDB state
23:06:15,666 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:06:15,787 INFO  state.RocksDBState             - RocksDB state backup complete
23:06:15,788 INFO  southpaw.Southpaw              - Building denormalized records
23:06:16,789 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:06:17,662 INFO  state.RocksDBState             - Backing up RocksDB state
23:06:17,663 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:06:17,780 INFO  state.RocksDBState             - RocksDB state backup complete
23:06:17,780 INFO  southpaw.Southpaw              - Building denormalized records
23:06:18,781 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:06:19,188 INFO  state.RocksDBState             - Backing up RocksDB state
23:06:19,188 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:06:19,296 INFO  state.RocksDBState             - RocksDB state backup complete
23:06:19,297 INFO  southpaw.Southpaw              - Building denormalized records
23:06:20,298 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:06:21,497 INFO  state.RocksDBState             - Backing up RocksDB state
23:06:21,497 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:06:21,617 INFO  state.RocksDBState             - RocksDB state backup complete
23:06:21,617 INFO  southpaw.Southpaw              - Building denormalized records
23:06:22,619 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:06:23,188 INFO  state.RocksDBState             - Backing up RocksDB state
23:06:23,188 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:06:23,285 INFO  state.RocksDBState             - RocksDB state backup complete
23:06:23,285 INFO  southpaw.Southpaw              - Building denormalized records
23:06:24,286 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:06:24,711 INFO  state.RocksDBState             - Backing up RocksDB state
23:06:24,712 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:06:24,794 INFO  state.RocksDBState             - RocksDB state backup complete
23:06:24,795 INFO  southpaw.Southpaw              - Building denormalized records
23:06:25,798 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:06:34,034 INFO  state.RocksDBState             - Backing up RocksDB state
23:06:34,034 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:06:34,175 INFO  state.RocksDBState             - RocksDB state backup complete
23:06:34,176 INFO  southpaw.Southpaw              - Building denormalized records
23:06:35,177 INFO  southpaw.Southpaw              - Performing a backup after a full commit
23:06:35,555 INFO  state.RocksDBState             - Backing up RocksDB state
23:06:35,555 INFO  state.RocksDBState             - Opening RocksDB backup engine
23:06:35,685 INFO  state.RocksDBState             - RocksDB state backup complete
23:06:35,693 INFO  state.RocksDBState             - Deleting RocksDB state backups
23:06:35,704 INFO  state.RocksDBState             - RocksDB state backups have been deleted
23:06:35,704 INFO  state.RocksDBState             - Deleting RocksDB state
23:06:35,709 INFO  state.RocksDBState             - RocksDB state has been deleted
Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 45.545 sec - in com.jwplayer.southpaw.SouthpawEndToEndTest
Running com.jwplayer.southpaw.topic.BlackHoleTopicTest
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.001 sec - in com.jwplayer.southpaw.topic.BlackHoleTopicTest
Running com.jwplayer.southpaw.topic.ConsoleTopicTest
key: A / value: B / filter mode: UPDATE
Tests run: 8, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.006 sec - in com.jwplayer.southpaw.topic.ConsoleTopicTest
Running com.jwplayer.southpaw.topic.KafkaTopicTest
23:06:35,944 INFO  server.ZooKeeperServerMain     - Starting server
23:06:35,949 INFO  server.ZooKeeperServer         - Server environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
23:06:35,949 INFO  server.ZooKeeperServer         - Server environment:host.name=ee25d8c76737
23:06:35,949 INFO  server.ZooKeeperServer         - Server environment:java.version=1.8.0_262
23:06:35,949 INFO  server.ZooKeeperServer         - Server environment:java.vendor=Oracle Corporation
23:06:35,949 INFO  server.ZooKeeperServer         - Server environment:java.home=/usr/local/openjdk-8/jre
23:06:35,949 INFO  server.ZooKeeperServer         - Server environment:java.class.path=/tmp/tmph9yrc09t/southpaw/target/test-classes:/tmp/tmph9yrc09t/southpaw/target/classes:/home/jdbl/.m2/repository/org/apache/logging/log4j/log4j-api/2.13.0/log4j-api-2.13.0.jar:/home/jdbl/.m2/repository/org/apache/logging/log4j/log4j-core/2.13.0/log4j-core-2.13.0.jar:/home/jdbl/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.0/log4j-slf4j-impl-2.13.0.jar:/home/jdbl/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/jdbl/.m2/repository/com/google/guava/guava/23.6-jre/guava-23.6-jre.jar:/home/jdbl/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/jdbl/.m2/repository/org/checkerframework/checker-compat-qual/2.0.0/checker-compat-qual-2.0.0.jar:/home/jdbl/.m2/repository/com/google/errorprone/error_prone_annotations/2.1.3/error_prone_annotations-2.1.3.jar:/home/jdbl/.m2/repository/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar:/home/jdbl/.m2/repository/org/codehaus/mojo/animal-sniffer-annotations/1.14/animal-sniffer-annotations-1.14.jar:/home/jdbl/.m2/repository/org/apache/commons/commons-collections4/4.1/commons-collections4-4.1.jar:/home/jdbl/.m2/repository/commons-io/commons-io/2.6/commons-io-2.6.jar:/home/jdbl/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/jdbl/.m2/repository/org/apache/kafka/kafka-streams/1.0.0/kafka-streams-1.0.0.jar:/home/jdbl/.m2/repository/org/apache/kafka/connect-json/1.0.0/connect-json-1.0.0.jar:/home/jdbl/.m2/repository/org/apache/kafka/connect-api/1.0.0/connect-api-1.0.0.jar:/home/jdbl/.m2/repository/org/apache/kafka/kafka-clients/1.0.0/kafka-clients-1.0.0.jar:/home/jdbl/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar:/home/jdbl/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/jdbl/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/jdbl/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/jdbl/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/jdbl/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar:/home/jdbl/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/jdbl/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/jdbl/.m2/repository/io/confluent/kafka-streams-avro-serde/4.0.0/kafka-streams-avro-serde-4.0.0.jar:/home/jdbl/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar:/home/jdbl/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar:/home/jdbl/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar:/home/jdbl/.m2/repository/io/confluent/kafka-json-serializer/4.0.0/kafka-json-serializer-4.0.0.jar:/home/jdbl/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar:/home/jdbl/.m2/repository/org/rocksdb/rocksdbjni/5.17.2/rocksdbjni-5.17.2.jar:/home/jdbl/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.8/jackson-core-2.9.8.jar:/home/jdbl/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.10.4/jackson-databind-2.9.10.4.jar:/home/jdbl/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.10/jackson-annotations-2.9.10.jar:/home/jdbl/.m2/repository/com/amazonaws/aws-java-sdk-s3/1.11.292/aws-java-sdk-s3-1.11.292.jar:/home/jdbl/.m2/repository/com/amazonaws/aws-java-sdk-kms/1.11.292/aws-java-sdk-kms-1.11.292.jar:/home/jdbl/.m2/repository/com/amazonaws/aws-java-sdk-core/1.11.292/aws-java-sdk-core-1.11.292.jar:/home/jdbl/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/jdbl/.m2/repository/org/apache/httpcomponents/httpclient/4.5.2/httpclient-4.5.2.jar:/home/jdbl/.m2/repository/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar:/home/jdbl/.m2/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/home/jdbl/.m2/repository/software/amazon/ion/ion-java/1.0.2/ion-java-1.0.2.jar:/home/jdbl/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.6.7/jackson-dataformat-cbor-2.6.7.jar:/home/jdbl/.m2/repository/joda-time/joda-time/2.8.1/joda-time-2.8.1.jar:/home/jdbl/.m2/repository/com/amazonaws/jmespath-java/1.11.292/jmespath-java-1.11.292.jar:/home/jdbl/.m2/repository/org/yaml/snakeyaml/1.20/snakeyaml-1.20.jar:/home/jdbl/.m2/repository/net/sf/jopt-simple/jopt-simple/4.9/jopt-simple-4.9.jar:/home/jdbl/.m2/repository/io/dropwizard/metrics/metrics-core/4.0.2/metrics-core-4.0.2.jar:/home/jdbl/.m2/repository/io/dropwizard/metrics/metrics-jmx/4.0.2/metrics-jmx-4.0.2.jar:/home/jdbl/.m2/repository/junit/junit/4.11/junit-4.11.jar:/home/jdbl/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/jdbl/.m2/repository/org/apache/kafka/kafka_2.12/1.0.0/kafka_2.12-1.0.0.jar:/home/jdbl/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/jdbl/.m2/repository/org/scala-lang/scala-library/2.12.3/scala-library-2.12.3.jar:/home/jdbl/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/jdbl/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar:/home/jdbl/.m2/repository/log4j/log4j/1.2.16/log4j-1.2.16.jar:/home/jdbl/.m2/repository/org/apache/curator/curator-test/2.12.0/curator-test-2.12.0.jar:/home/jdbl/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/jdbl/.m2/repository/io/findify/s3mock_2.12/0.2.5/s3mock_2.12-0.2.5.jar:/home/jdbl/.m2/repository/com/typesafe/akka/akka-stream_2.12/2.5.11/akka-stream_2.12-2.5.11.jar:/home/jdbl/.m2/repository/com/typesafe/akka/akka-actor_2.12/2.5.11/akka-actor_2.12-2.5.11.jar:/home/jdbl/.m2/repository/com/typesafe/config/1.3.2/config-1.3.2.jar:/home/jdbl/.m2/repository/org/scala-lang/modules/scala-java8-compat_2.12/0.8.0/scala-java8-compat_2.12-0.8.0.jar:/home/jdbl/.m2/repository/com/typesafe/akka/akka-protobuf_2.12/2.5.11/akka-protobuf_2.12-2.5.11.jar:/home/jdbl/.m2/repository/org/reactivestreams/reactive-streams/1.0.2/reactive-streams-1.0.2.jar:/home/jdbl/.m2/repository/com/typesafe/ssl-config-core_2.12/0.2.2/ssl-config-core_2.12-0.2.2.jar:/home/jdbl/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.12/1.0.4/scala-parser-combinators_2.12-1.0.4.jar:/home/jdbl/.m2/repository/com/typesafe/akka/akka-http_2.12/10.1.0/akka-http_2.12-10.1.0.jar:/home/jdbl/.m2/repository/com/typesafe/akka/akka-http-core_2.12/10.1.0/akka-http-core_2.12-10.1.0.jar:/home/jdbl/.m2/repository/com/typesafe/akka/akka-parsing_2.12/10.1.0/akka-parsing_2.12-10.1.0.jar:/home/jdbl/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.1.0/scala-xml_2.12-1.1.0.jar:/home/jdbl/.m2/repository/com/github/pathikrit/better-files_2.12/3.4.0/better-files_2.12-3.4.0.jar:/home/jdbl/.m2/repository/com/typesafe/scala-logging/scala-logging_2.12/3.8.0/scala-logging_2.12-3.8.0.jar:/home/jdbl/.m2/repository/org/scala-lang/scala-reflect/2.12.4/scala-reflect-2.12.4.jar:/home/jdbl/.m2/repository/org/iq80/leveldb/leveldb/0.10/leveldb-0.10.jar:/home/jdbl/.m2/repository/org/iq80/leveldb/leveldb-api/0.10/leveldb-api-0.10.jar:/home/jdbl/.m2/repository/org/mockito/mockito-core/2.25.1/mockito-core-2.25.1.jar:/home/jdbl/.m2/repository/net/bytebuddy/byte-buddy/1.9.7/byte-buddy-1.9.7.jar:/home/jdbl/.m2/repository/net/bytebuddy/byte-buddy-agent/1.9.7/byte-buddy-agent-1.9.7.jar:/home/jdbl/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:
23:06:35,950 INFO  server.ZooKeeperServer         - Server environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
23:06:35,950 INFO  server.ZooKeeperServer         - Server environment:java.io.tmpdir=/tmp
23:06:35,950 INFO  server.ZooKeeperServer         - Server environment:java.compiler=<NA>
23:06:35,950 INFO  server.ZooKeeperServer         - Server environment:os.name=Linux
23:06:35,950 INFO  server.ZooKeeperServer         - Server environment:os.arch=amd64
23:06:35,950 INFO  server.ZooKeeperServer         - Server environment:os.version=4.15.0-109-generic
23:06:35,950 INFO  server.ZooKeeperServer         - Server environment:user.name=jdbl
23:06:35,950 INFO  server.ZooKeeperServer         - Server environment:user.home=/home/jdbl
23:06:35,950 INFO  server.ZooKeeperServer         - Server environment:user.dir=/tmp/tmph9yrc09t/southpaw
23:06:35,958 INFO  server.ZooKeeperServer         - tickTime set to 3000
23:06:35,958 INFO  server.ZooKeeperServer         - minSessionTimeout set to -1
23:06:35,958 INFO  server.ZooKeeperServer         - maxSessionTimeout set to -1
23:06:35,974 INFO  server.NIOServerCnxnFactory    - binding to port 0.0.0.0/0.0.0.0:35697
23:06:37,065 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:06:37,073 INFO  zookeeper.ZooKeeper            - Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
23:06:37,073 INFO  zookeeper.ZooKeeper            - Client environment:host.name=ee25d8c76737
23:06:37,073 INFO  zookeeper.ZooKeeper            - Client environment:java.version=1.8.0_262
23:06:37,073 INFO  zookeeper.ZooKeeper            - Client environment:java.vendor=Oracle Corporation
23:06:37,073 INFO  zookeeper.ZooKeeper            - Client environment:java.home=/usr/local/openjdk-8/jre
23:06:37,073 INFO  zookeeper.ZooKeeper            - Client environment:java.class.path=/tmp/tmph9yrc09t/southpaw/target/test-classes:/tmp/tmph9yrc09t/southpaw/target/classes:/home/jdbl/.m2/repository/org/apache/logging/log4j/log4j-api/2.13.0/log4j-api-2.13.0.jar:/home/jdbl/.m2/repository/org/apache/logging/log4j/log4j-core/2.13.0/log4j-core-2.13.0.jar:/home/jdbl/.m2/repository/org/apache/logging/log4j/log4j-slf4j-impl/2.13.0/log4j-slf4j-impl-2.13.0.jar:/home/jdbl/.m2/repository/org/slf4j/slf4j-api/1.7.25/slf4j-api-1.7.25.jar:/home/jdbl/.m2/repository/com/google/guava/guava/23.6-jre/guava-23.6-jre.jar:/home/jdbl/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/home/jdbl/.m2/repository/org/checkerframework/checker-compat-qual/2.0.0/checker-compat-qual-2.0.0.jar:/home/jdbl/.m2/repository/com/google/errorprone/error_prone_annotations/2.1.3/error_prone_annotations-2.1.3.jar:/home/jdbl/.m2/repository/com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar:/home/jdbl/.m2/repository/org/codehaus/mojo/animal-sniffer-annotations/1.14/animal-sniffer-annotations-1.14.jar:/home/jdbl/.m2/repository/org/apache/commons/commons-collections4/4.1/commons-collections4-4.1.jar:/home/jdbl/.m2/repository/commons-io/commons-io/2.6/commons-io-2.6.jar:/home/jdbl/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar:/home/jdbl/.m2/repository/org/apache/kafka/kafka-streams/1.0.0/kafka-streams-1.0.0.jar:/home/jdbl/.m2/repository/org/apache/kafka/connect-json/1.0.0/connect-json-1.0.0.jar:/home/jdbl/.m2/repository/org/apache/kafka/connect-api/1.0.0/connect-api-1.0.0.jar:/home/jdbl/.m2/repository/org/apache/kafka/kafka-clients/1.0.0/kafka-clients-1.0.0.jar:/home/jdbl/.m2/repository/org/lz4/lz4-java/1.4/lz4-java-1.4.jar:/home/jdbl/.m2/repository/org/xerial/snappy/snappy-java/1.1.4/snappy-java-1.1.4.jar:/home/jdbl/.m2/repository/org/apache/avro/avro/1.8.2/avro-1.8.2.jar:/home/jdbl/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/home/jdbl/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/home/jdbl/.m2/repository/com/thoughtworks/paranamer/paranamer/2.7/paranamer-2.7.jar:/home/jdbl/.m2/repository/org/apache/commons/commons-compress/1.8.1/commons-compress-1.8.1.jar:/home/jdbl/.m2/repository/org/tukaani/xz/1.5/xz-1.5.jar:/home/jdbl/.m2/repository/io/confluent/kafka-streams-avro-serde/4.0.0/kafka-streams-avro-serde-4.0.0.jar:/home/jdbl/.m2/repository/io/confluent/kafka-avro-serializer/4.0.0/kafka-avro-serializer-4.0.0.jar:/home/jdbl/.m2/repository/io/confluent/kafka-schema-registry-client/4.0.0/kafka-schema-registry-client-4.0.0.jar:/home/jdbl/.m2/repository/io/confluent/common-utils/4.0.0/common-utils-4.0.0.jar:/home/jdbl/.m2/repository/io/confluent/kafka-json-serializer/4.0.0/kafka-json-serializer-4.0.0.jar:/home/jdbl/.m2/repository/io/confluent/common-config/4.0.0/common-config-4.0.0.jar:/home/jdbl/.m2/repository/org/rocksdb/rocksdbjni/5.17.2/rocksdbjni-5.17.2.jar:/home/jdbl/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.9.8/jackson-core-2.9.8.jar:/home/jdbl/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.9.10.4/jackson-databind-2.9.10.4.jar:/home/jdbl/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.9.10/jackson-annotations-2.9.10.jar:/home/jdbl/.m2/repository/com/amazonaws/aws-java-sdk-s3/1.11.292/aws-java-sdk-s3-1.11.292.jar:/home/jdbl/.m2/repository/com/amazonaws/aws-java-sdk-kms/1.11.292/aws-java-sdk-kms-1.11.292.jar:/home/jdbl/.m2/repository/com/amazonaws/aws-java-sdk-core/1.11.292/aws-java-sdk-core-1.11.292.jar:/home/jdbl/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/home/jdbl/.m2/repository/org/apache/httpcomponents/httpclient/4.5.2/httpclient-4.5.2.jar:/home/jdbl/.m2/repository/org/apache/httpcomponents/httpcore/4.4.4/httpcore-4.4.4.jar:/home/jdbl/.m2/repository/commons-codec/commons-codec/1.9/commons-codec-1.9.jar:/home/jdbl/.m2/repository/software/amazon/ion/ion-java/1.0.2/ion-java-1.0.2.jar:/home/jdbl/.m2/repository/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.6.7/jackson-dataformat-cbor-2.6.7.jar:/home/jdbl/.m2/repository/joda-time/joda-time/2.8.1/joda-time-2.8.1.jar:/home/jdbl/.m2/repository/com/amazonaws/jmespath-java/1.11.292/jmespath-java-1.11.292.jar:/home/jdbl/.m2/repository/org/yaml/snakeyaml/1.20/snakeyaml-1.20.jar:/home/jdbl/.m2/repository/net/sf/jopt-simple/jopt-simple/4.9/jopt-simple-4.9.jar:/home/jdbl/.m2/repository/io/dropwizard/metrics/metrics-core/4.0.2/metrics-core-4.0.2.jar:/home/jdbl/.m2/repository/io/dropwizard/metrics/metrics-jmx/4.0.2/metrics-jmx-4.0.2.jar:/home/jdbl/.m2/repository/junit/junit/4.11/junit-4.11.jar:/home/jdbl/.m2/repository/org/hamcrest/hamcrest-core/1.3/hamcrest-core-1.3.jar:/home/jdbl/.m2/repository/org/apache/kafka/kafka_2.12/1.0.0/kafka_2.12-1.0.0.jar:/home/jdbl/.m2/repository/com/yammer/metrics/metrics-core/2.2.0/metrics-core-2.2.0.jar:/home/jdbl/.m2/repository/org/scala-lang/scala-library/2.12.3/scala-library-2.12.3.jar:/home/jdbl/.m2/repository/com/101tec/zkclient/0.10/zkclient-0.10.jar:/home/jdbl/.m2/repository/org/apache/zookeeper/zookeeper/3.4.10/zookeeper-3.4.10.jar:/home/jdbl/.m2/repository/log4j/log4j/1.2.16/log4j-1.2.16.jar:/home/jdbl/.m2/repository/org/apache/curator/curator-test/2.12.0/curator-test-2.12.0.jar:/home/jdbl/.m2/repository/org/javassist/javassist/3.18.1-GA/javassist-3.18.1-GA.jar:/home/jdbl/.m2/repository/io/findify/s3mock_2.12/0.2.5/s3mock_2.12-0.2.5.jar:/home/jdbl/.m2/repository/com/typesafe/akka/akka-stream_2.12/2.5.11/akka-stream_2.12-2.5.11.jar:/home/jdbl/.m2/repository/com/typesafe/akka/akka-actor_2.12/2.5.11/akka-actor_2.12-2.5.11.jar:/home/jdbl/.m2/repository/com/typesafe/config/1.3.2/config-1.3.2.jar:/home/jdbl/.m2/repository/org/scala-lang/modules/scala-java8-compat_2.12/0.8.0/scala-java8-compat_2.12-0.8.0.jar:/home/jdbl/.m2/repository/com/typesafe/akka/akka-protobuf_2.12/2.5.11/akka-protobuf_2.12-2.5.11.jar:/home/jdbl/.m2/repository/org/reactivestreams/reactive-streams/1.0.2/reactive-streams-1.0.2.jar:/home/jdbl/.m2/repository/com/typesafe/ssl-config-core_2.12/0.2.2/ssl-config-core_2.12-0.2.2.jar:/home/jdbl/.m2/repository/org/scala-lang/modules/scala-parser-combinators_2.12/1.0.4/scala-parser-combinators_2.12-1.0.4.jar:/home/jdbl/.m2/repository/com/typesafe/akka/akka-http_2.12/10.1.0/akka-http_2.12-10.1.0.jar:/home/jdbl/.m2/repository/com/typesafe/akka/akka-http-core_2.12/10.1.0/akka-http-core_2.12-10.1.0.jar:/home/jdbl/.m2/repository/com/typesafe/akka/akka-parsing_2.12/10.1.0/akka-parsing_2.12-10.1.0.jar:/home/jdbl/.m2/repository/org/scala-lang/modules/scala-xml_2.12/1.1.0/scala-xml_2.12-1.1.0.jar:/home/jdbl/.m2/repository/com/github/pathikrit/better-files_2.12/3.4.0/better-files_2.12-3.4.0.jar:/home/jdbl/.m2/repository/com/typesafe/scala-logging/scala-logging_2.12/3.8.0/scala-logging_2.12-3.8.0.jar:/home/jdbl/.m2/repository/org/scala-lang/scala-reflect/2.12.4/scala-reflect-2.12.4.jar:/home/jdbl/.m2/repository/org/iq80/leveldb/leveldb/0.10/leveldb-0.10.jar:/home/jdbl/.m2/repository/org/iq80/leveldb/leveldb-api/0.10/leveldb-api-0.10.jar:/home/jdbl/.m2/repository/org/mockito/mockito-core/2.25.1/mockito-core-2.25.1.jar:/home/jdbl/.m2/repository/net/bytebuddy/byte-buddy/1.9.7/byte-buddy-1.9.7.jar:/home/jdbl/.m2/repository/net/bytebuddy/byte-buddy-agent/1.9.7/byte-buddy-agent-1.9.7.jar:/home/jdbl/.m2/repository/org/objenesis/objenesis/2.6/objenesis-2.6.jar:
23:06:37,074 INFO  zookeeper.ZooKeeper            - Client environment:java.library.path=/usr/java/packages/lib/amd64:/usr/lib64:/lib64:/lib:/usr/lib
23:06:37,074 INFO  zookeeper.ZooKeeper            - Client environment:java.io.tmpdir=/tmp
23:06:37,074 INFO  zookeeper.ZooKeeper            - Client environment:java.compiler=<NA>
23:06:37,074 INFO  zookeeper.ZooKeeper            - Client environment:os.name=Linux
23:06:37,074 INFO  zookeeper.ZooKeeper            - Client environment:os.arch=amd64
23:06:37,074 INFO  zookeeper.ZooKeeper            - Client environment:os.version=4.15.0-109-generic
23:06:37,074 INFO  zookeeper.ZooKeeper            - Client environment:user.name=jdbl
23:06:37,074 INFO  zookeeper.ZooKeeper            - Client environment:user.home=/home/jdbl
23:06:37,074 INFO  zookeeper.ZooKeeper            - Client environment:user.dir=/tmp/tmph9yrc09t/southpaw
23:06:37,076 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:35697 sessionTimeout=2147483647 watcher=org.I0Itec.zkclient.ZkClient@73453784
23:06:37,109 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:06:37,120 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:06:37,121 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:35697, initiating session
23:06:37,121 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:51636
23:06:37,137 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:51636
23:06:37,141 INFO  persistence.FileTxnLog         - Creating new log file: log.1
23:06:37,169 INFO  server.ZooKeeperServer         - Established session 0x173a1fa347b0000 with negotiated timeout 60000 for client /127.0.0.1:51636
23:06:37,169 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:35697, sessionid = 0x173a1fa347b0000, negotiated timeout = 60000
23:06:37,173 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:06:37,309 INFO  server.KafkaConfig             - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka/logs/log-43629
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 43629
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:35697
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

23:06:37,394 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:35697 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@6e730be6
23:06:37,397 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:06:37,401 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:06:37,405 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:06:37,417 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:35697, initiating session
23:06:37,417 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:51648
23:06:37,417 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:51648
23:06:37,420 INFO  server.ZooKeeperServer         - Established session 0x173a1fa347b0001 with negotiated timeout 6000 for client /127.0.0.1:51648
23:06:37,425 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:35697, sessionid = 0x173a1fa347b0001, negotiated timeout = 6000
23:06:37,425 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:06:37,511 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa347b0001 type:create cxid:0x5 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
23:06:37,545 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa347b0001 type:create cxid:0xb zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
23:06:37,569 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa347b0001 type:create cxid:0x13 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
23:06:37,625 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa347b0001 type:create cxid:0x1f zxid:0x14 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
23:06:38,785 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa347b0001 type:setData cxid:0x2a zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
23:06:39,084 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa347b0001 type:create cxid:0x4a zxid:0x1b txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
23:06:39,105 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa347b0001 type:create cxid:0x4b zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
23:06:39,106 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa347b0001 type:delete cxid:0x4d zxid:0x1e txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
23:06:39,159 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:39,160 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:39,222 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa347b0000 type:setData cxid:0x5 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics/test-topic Error:KeeperErrorCode = NoNode for /config/topics/test-topic
23:06:39,225 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa347b0000 type:create cxid:0x7 zxid:0x20 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
23:06:39,243 WARN  topic.KafkaTopic               - Since Southpaw handles its own offsets, the auto offset reset config is ignored. If there are no existing offsets, we will always start at the beginning.
23:06:39,273 INFO  consumer.ConsumerConfig        - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:43629]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

23:06:39,277 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa347b0001 type:create cxid:0x57 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions/0
23:06:39,280 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa347b0001 type:create cxid:0x58 zxid:0x24 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions
23:06:39,358 WARN  consumer.ConsumerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:39,358 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:39,358 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:39,539 INFO  topic.KafkaTopic               - No offsets found for topic test, seeking to beginning.
23:06:39,539 WARN  topic.KafkaTopic               - It is recommended to set ACKS to 'all' otherwise data loss can occur
23:06:39,553 INFO  producer.ProducerConfig        - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:43629]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

23:06:39,581 WARN  producer.ProducerConfig        - The configuration 'enable.auto.commit' was supplied but isn't a known config.
23:06:39,581 WARN  producer.ProducerConfig        - The configuration 'group.id' was supplied but isn't a known config.
23:06:39,581 WARN  producer.ProducerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:39,581 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:39,581 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:39,753 WARN  topic.KafkaTopic               - Since Southpaw handles its own offsets, the auto offset reset config is ignored. If there are no existing offsets, we will always start at the beginning.
23:06:39,753 INFO  consumer.ConsumerConfig        - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:43629]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

23:06:39,756 WARN  consumer.ConsumerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:39,757 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:39,757 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:39,791 INFO  topic.KafkaTopic               - No offsets found for topic test, seeking to beginning.
23:06:39,791 WARN  topic.KafkaTopic               - It is recommended to set ACKS to 'all' otherwise data loss can occur
23:06:39,791 INFO  producer.ProducerConfig        - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:43629]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

23:06:39,796 WARN  producer.ProducerConfig        - The configuration 'enable.auto.commit' was supplied but isn't a known config.
23:06:39,796 WARN  producer.ProducerConfig        - The configuration 'group.id' was supplied but isn't a known config.
23:06:39,796 WARN  producer.ProducerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:39,796 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:39,796 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:39,940 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:39,970 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:40,071 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:40,092 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:40,322 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:40,343 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:40,696 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:40,725 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:41,480 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:41,500 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:42,369 INFO  zkclient.ZkEventThread         - Terminate ZkClient event thread.
23:06:42,370 INFO  server.PrepRequestProcessor    - Processed session termination for sessionid: 0x173a1fa347b0001
23:06:42,374 INFO  zookeeper.ZooKeeper            - Session: 0x173a1fa347b0001 closed
23:06:42,374 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:51648 which had sessionid 0x173a1fa347b0001
23:06:42,374 INFO  zookeeper.ClientCnxn           - EventThread shut down for session: 0x173a1fa347b0001
23:06:42,381 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:51636 which had sessionid 0x173a1fa347b0000
23:06:42,381 INFO  server.NIOServerCnxnFactory    - NIOServerCnxn factory exited run method
23:06:42,381 INFO  zookeeper.ClientCnxn           - Unable to read additional data from server sessionid 0x173a1fa347b0000, likely server has closed socket, closing socket connection and attempting reconnect
23:06:42,382 INFO  server.ZooKeeperServer         - shutting down
23:06:42,382 INFO  server.SessionTrackerImpl      - Shutting down
23:06:42,382 INFO  server.PrepRequestProcessor    - Shutting down
23:06:42,382 INFO  server.PrepRequestProcessor    - PrepRequestProcessor exited loop!
23:06:42,382 INFO  server.SyncRequestProcessor    - Shutting down
23:06:42,382 INFO  server.SyncRequestProcessor    - SyncRequestProcessor exited!
23:06:42,383 INFO  server.FinalRequestProcessor   - shutdown of request processor complete
23:06:42,389 INFO  server.ZooKeeperServerMain     - Starting server
23:06:42,389 INFO  server.ZooKeeperServer         - tickTime set to 3000
23:06:42,389 INFO  server.ZooKeeperServer         - minSessionTimeout set to -1
23:06:42,389 INFO  server.ZooKeeperServer         - maxSessionTimeout set to -1
23:06:42,390 INFO  server.NIOServerCnxnFactory    - binding to port 0.0.0.0/0.0.0.0:36045
23:06:42,485 INFO  zkclient.ZkClient              - zookeeper state changed (Disconnected)
23:06:42,487 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:42,567 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:43,421 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:36045 sessionTimeout=2147483647 watcher=org.I0Itec.zkclient.ZkClient@5d29caa
23:06:43,421 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:06:43,429 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:06:43,433 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:06:43,433 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:36045, initiating session
23:06:43,433 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:32820
23:06:43,434 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:32820
23:06:43,437 INFO  persistence.FileTxnLog         - Creating new log file: log.1
23:06:43,442 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:43,586 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:43,617 INFO  server.ZooKeeperServer         - Established session 0x173a1fa4d5b0000 with negotiated timeout 60000 for client /127.0.0.1:32820
23:06:43,617 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:36045, sessionid = 0x173a1fa4d5b0000, negotiated timeout = 60000
23:06:43,618 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:06:43,619 INFO  server.KafkaConfig             - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka/logs/log-43201
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 43201
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:36045
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

23:06:43,621 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:36045 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@79daf7e3
23:06:43,621 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:06:43,641 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:06:43,641 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:06:43,641 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:36045, initiating session
23:06:43,641 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:32832
23:06:43,641 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:32832
23:06:43,719 INFO  server.ZooKeeperServer         - Established session 0x173a1fa4d5b0001 with negotiated timeout 6000 for client /127.0.0.1:32832
23:06:43,719 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:36045, sessionid = 0x173a1fa4d5b0001, negotiated timeout = 6000
23:06:43,719 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:06:43,814 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa4d5b0001 type:create cxid:0x5 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
23:06:43,987 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa4d5b0001 type:create cxid:0xb zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
23:06:44,034 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa4d5b0001 type:create cxid:0x13 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
23:06:44,069 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa4d5b0001 type:create cxid:0x1f zxid:0x14 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
23:06:44,623 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:06:44,624 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:44,630 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa347b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:44,631 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:44,745 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa4d5b0001 type:setData cxid:0x2a zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
23:06:44,766 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa4d5b0001 type:delete cxid:0x41 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
23:06:44,793 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa4d5b0001 type:create cxid:0x4b zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
23:06:44,793 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa4d5b0001 type:create cxid:0x4c zxid:0x1d txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
23:06:44,806 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:44,806 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:44,810 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa4d5b0000 type:setData cxid:0x5 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics/test-topic Error:KeeperErrorCode = NoNode for /config/topics/test-topic
23:06:44,813 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa4d5b0000 type:create cxid:0x7 zxid:0x20 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
23:06:44,825 WARN  topic.KafkaTopic               - Since Southpaw handles its own offsets, the auto offset reset config is ignored. If there are no existing offsets, we will always start at the beginning.
23:06:44,826 INFO  consumer.ConsumerConfig        - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:43201]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

23:06:44,840 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa4d5b0001 type:create cxid:0x57 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions/0
23:06:44,842 WARN  consumer.ConsumerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:44,842 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:44,842 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:44,843 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa4d5b0001 type:create cxid:0x58 zxid:0x24 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions
23:06:44,962 INFO  topic.KafkaTopic               - No offsets found for topic test, seeking to beginning.
23:06:44,963 WARN  topic.KafkaTopic               - It is recommended to set ACKS to 'all' otherwise data loss can occur
23:06:44,963 INFO  producer.ProducerConfig        - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:43201]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

23:06:44,967 WARN  producer.ProducerConfig        - The configuration 'enable.auto.commit' was supplied but isn't a known config.
23:06:44,967 WARN  producer.ProducerConfig        - The configuration 'group.id' was supplied but isn't a known config.
23:06:44,967 WARN  producer.ProducerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:44,967 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:44,967 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:45,000 INFO  server.SessionTrackerImpl      - SessionTrackerImpl exited loop!
23:06:45,122 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:45,223 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:45,425 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:45,585 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:45,829 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:45,927 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:46,171 INFO  zkclient.ZkEventThread         - Terminate ZkClient event thread.
23:06:46,172 INFO  server.PrepRequestProcessor    - Processed session termination for sessionid: 0x173a1fa4d5b0001
23:06:46,177 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:32832 which had sessionid 0x173a1fa4d5b0001
23:06:46,177 INFO  zookeeper.ZooKeeper            - Session: 0x173a1fa4d5b0001 closed
23:06:46,177 INFO  zookeeper.ClientCnxn           - EventThread shut down for session: 0x173a1fa4d5b0001
23:06:46,185 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:32820 which had sessionid 0x173a1fa4d5b0000
23:06:46,185 INFO  server.NIOServerCnxnFactory    - NIOServerCnxn factory exited run method
23:06:46,185 INFO  zookeeper.ClientCnxn           - Unable to read additional data from server sessionid 0x173a1fa4d5b0000, likely server has closed socket, closing socket connection and attempting reconnect
23:06:46,185 INFO  server.ZooKeeperServer         - shutting down
23:06:46,186 INFO  server.SessionTrackerImpl      - Shutting down
23:06:46,186 INFO  server.PrepRequestProcessor    - Shutting down
23:06:46,186 INFO  server.PrepRequestProcessor    - PrepRequestProcessor exited loop!
23:06:46,186 INFO  server.SyncRequestProcessor    - Shutting down
23:06:46,186 INFO  server.SyncRequestProcessor    - SyncRequestProcessor exited!
23:06:46,186 INFO  server.FinalRequestProcessor   - shutdown of request processor complete
23:06:46,190 INFO  server.ZooKeeperServerMain     - Starting server
23:06:46,190 INFO  server.ZooKeeperServer         - tickTime set to 3000
23:06:46,190 INFO  server.ZooKeeperServer         - minSessionTimeout set to -1
23:06:46,190 INFO  server.ZooKeeperServer         - maxSessionTimeout set to -1
23:06:46,190 INFO  server.NIOServerCnxnFactory    - binding to port 0.0.0.0/0.0.0.0:39195
23:06:46,286 INFO  zkclient.ZkClient              - zookeeper state changed (Disconnected)
23:06:46,618 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:06:46,619 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa347b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:46,789 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:46,794 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:46,992 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:47,213 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:39195 sessionTimeout=2147483647 watcher=org.I0Itec.zkclient.ZkClient@683e36b1
23:06:47,213 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:06:47,225 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:06:47,229 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:39195. Will not attempt to authenticate using SASL (unknown error)
23:06:47,229 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:39195, initiating session
23:06:47,233 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:49516
23:06:47,233 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:49516
23:06:47,241 INFO  persistence.FileTxnLog         - Creating new log file: log.1
23:06:47,252 INFO  server.ZooKeeperServer         - Established session 0x173a1fa5c350000 with negotiated timeout 60000 for client /127.0.0.1:49516
23:06:47,257 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:39195, sessionid = 0x173a1fa5c350000, negotiated timeout = 60000
23:06:47,261 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:06:47,262 INFO  server.KafkaConfig             - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka/logs/log-36211
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 36211
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:39195
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

23:06:47,273 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:39195 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@509e5cea
23:06:47,277 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:06:47,281 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:06:47,285 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:39195. Will not attempt to authenticate using SASL (unknown error)
23:06:47,285 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:39195, initiating session
23:06:47,285 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:49522
23:06:47,285 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:49522
23:06:47,293 INFO  server.ZooKeeperServer         - Established session 0x173a1fa5c350001 with negotiated timeout 6000 for client /127.0.0.1:49522
23:06:47,296 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:39195, sessionid = 0x173a1fa5c350001, negotiated timeout = 6000
23:06:47,297 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:06:47,385 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa5c350001 type:create cxid:0x5 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
23:06:47,397 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:06:47,397 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa4d5b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:47,449 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa5c350001 type:create cxid:0xb zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
23:06:47,505 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa5c350001 type:create cxid:0x13 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
23:06:47,547 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa5c350001 type:create cxid:0x1f zxid:0x14 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
23:06:47,694 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:47,709 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:47,937 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa5c350001 type:setData cxid:0x2d zxid:0x19 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
23:06:47,960 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa5c350001 type:delete cxid:0x41 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
23:06:48,001 INFO  server.SessionTrackerImpl      - SessionTrackerImpl exited loop!
23:06:48,036 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa5c350001 type:create cxid:0x4b zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
23:06:48,045 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa5c350001 type:create cxid:0x4c zxid:0x1d txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
23:06:48,059 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:06:48,059 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa347b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:48,070 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:48,070 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:48,093 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa5c350000 type:setData cxid:0x5 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics/test-topic Error:KeeperErrorCode = NoNode for /config/topics/test-topic
23:06:48,142 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa5c350000 type:create cxid:0x7 zxid:0x20 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
23:06:48,180 WARN  topic.KafkaTopic               - Since Southpaw handles its own offsets, the auto offset reset config is ignored. If there are no existing offsets, we will always start at the beginning.
23:06:48,180 INFO  consumer.ConsumerConfig        - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:36211]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

23:06:48,181 WARN  consumer.ConsumerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:48,181 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:48,181 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:48,205 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa5c350001 type:create cxid:0x57 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions/0
23:06:48,213 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa5c350001 type:create cxid:0x59 zxid:0x24 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions
23:06:48,229 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:48,346 INFO  topic.KafkaTopic               - No offsets found for topic test, seeking to beginning.
23:06:48,346 WARN  topic.KafkaTopic               - It is recommended to set ACKS to 'all' otherwise data loss can occur
23:06:48,346 INFO  producer.ProducerConfig        - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:36211]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

23:06:48,357 WARN  producer.ProducerConfig        - The configuration 'enable.auto.commit' was supplied but isn't a known config.
23:06:48,357 WARN  producer.ProducerConfig        - The configuration 'group.id' was supplied but isn't a known config.
23:06:48,357 WARN  producer.ProducerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:48,357 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:48,357 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:48,425 INFO  topic.KafkaTopic               - Resetting offsets for topic test, seeking to beginning.
23:06:48,526 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:48,629 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:48,649 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:48,683 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:48,690 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:06:48,690 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa4d5b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:48,880 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:49,233 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:49,336 INFO  zkclient.ZkEventThread         - Terminate ZkClient event thread.
23:06:49,337 INFO  server.PrepRequestProcessor    - Processed session termination for sessionid: 0x173a1fa5c350001
23:06:49,340 INFO  zookeeper.ZooKeeper            - Session: 0x173a1fa5c350001 closed
23:06:49,340 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:49522 which had sessionid 0x173a1fa5c350001
23:06:49,341 INFO  zookeeper.ClientCnxn           - EventThread shut down for session: 0x173a1fa5c350001
23:06:49,349 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:49516 which had sessionid 0x173a1fa5c350000
23:06:49,349 INFO  zookeeper.ClientCnxn           - Unable to read additional data from server sessionid 0x173a1fa5c350000, likely server has closed socket, closing socket connection and attempting reconnect
23:06:49,349 INFO  server.NIOServerCnxnFactory    - NIOServerCnxn factory exited run method
23:06:49,349 INFO  server.ZooKeeperServer         - shutting down
23:06:49,349 INFO  server.SessionTrackerImpl      - Shutting down
23:06:49,350 INFO  server.PrepRequestProcessor    - Shutting down
23:06:49,350 INFO  server.SyncRequestProcessor    - Shutting down
23:06:49,350 INFO  server.PrepRequestProcessor    - PrepRequestProcessor exited loop!
23:06:49,350 INFO  server.SyncRequestProcessor    - SyncRequestProcessor exited!
23:06:49,350 INFO  server.FinalRequestProcessor   - shutdown of request processor complete
23:06:49,357 INFO  server.ZooKeeperServerMain     - Starting server
23:06:49,357 INFO  server.ZooKeeperServer         - tickTime set to 3000
23:06:49,357 INFO  server.ZooKeeperServer         - minSessionTimeout set to -1
23:06:49,357 INFO  server.ZooKeeperServer         - maxSessionTimeout set to -1
23:06:49,358 INFO  server.NIOServerCnxnFactory    - binding to port 0.0.0.0/0.0.0.0:46551
23:06:49,412 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:49,453 INFO  zkclient.ZkClient              - zookeeper state changed (Disconnected)
23:06:49,687 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:49,722 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:49,882 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:06:49,883 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa4d5b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:49,964 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:06:49,964 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa347b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:50,239 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:50,397 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:46551 sessionTimeout=2147483647 watcher=org.I0Itec.zkclient.ZkClient@1ca3d310
23:06:50,397 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:06:50,400 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:06:50,401 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:46551. Will not attempt to authenticate using SASL (unknown error)
23:06:50,401 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:46551, initiating session
23:06:50,401 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:33926
23:06:50,402 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:33926
23:06:50,402 INFO  persistence.FileTxnLog         - Creating new log file: log.1
23:06:50,413 INFO  server.ZooKeeperServer         - Established session 0x173a1fa68910000 with negotiated timeout 60000 for client /127.0.0.1:33926
23:06:50,413 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:46551, sessionid = 0x173a1fa68910000, negotiated timeout = 60000
23:06:50,417 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:06:50,417 INFO  server.KafkaConfig             - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka/logs/log-34269
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 34269
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:46551
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

23:06:50,421 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:46551 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@325e5052
23:06:50,421 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:06:50,429 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:06:50,433 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:46551. Will not attempt to authenticate using SASL (unknown error)
23:06:50,437 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:46551, initiating session
23:06:50,438 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:33928
23:06:50,438 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:33928
23:06:50,444 INFO  server.ZooKeeperServer         - Established session 0x173a1fa68910001 with negotiated timeout 6000 for client /127.0.0.1:33928
23:06:50,444 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:46551, sessionid = 0x173a1fa68910001, negotiated timeout = 6000
23:06:50,446 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:06:50,473 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910001 type:create cxid:0x5 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
23:06:50,496 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910001 type:create cxid:0xb zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
23:06:50,517 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910001 type:create cxid:0x13 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
23:06:50,522 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:50,566 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910001 type:create cxid:0x1f zxid:0x14 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
23:06:51,081 INFO  server.SessionTrackerImpl      - SessionTrackerImpl exited loop!
23:06:51,089 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:51,093 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:51,093 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:51,145 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:39195. Will not attempt to authenticate using SASL (unknown error)
23:06:51,145 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa5c350000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:51,381 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910001 type:setData cxid:0x2c zxid:0x19 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
23:06:51,435 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:06:51,436 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa4d5b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:51,525 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910001 type:delete cxid:0x4a zxid:0x1b txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
23:06:51,533 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910001 type:create cxid:0x4b zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
23:06:51,533 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910001 type:create cxid:0x4c zxid:0x1d txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
23:06:51,555 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:51,555 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:51,574 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910000 type:setData cxid:0x5 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics/test-topic Error:KeeperErrorCode = NoNode for /config/topics/test-topic
23:06:51,578 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910000 type:create cxid:0x7 zxid:0x20 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
23:06:51,597 WARN  topic.KafkaTopic               - Since Southpaw handles its own offsets, the auto offset reset config is ignored. If there are no existing offsets, we will always start at the beginning.
23:06:51,597 INFO  consumer.ConsumerConfig        - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:34269]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

23:06:51,599 WARN  consumer.ConsumerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:51,599 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:51,599 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:51,611 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910001 type:create cxid:0x57 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions/0
23:06:51,623 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910001 type:create cxid:0x59 zxid:0x24 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions
23:06:51,653 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:51,753 INFO  topic.KafkaTopic               - No offsets found for topic test, seeking to beginning.
23:06:51,753 WARN  topic.KafkaTopic               - It is recommended to set ACKS to 'all' otherwise data loss can occur
23:06:51,753 INFO  producer.ProducerConfig        - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:34269]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

23:06:51,757 WARN  producer.ProducerConfig        - The configuration 'enable.auto.commit' was supplied but isn't a known config.
23:06:51,757 WARN  producer.ProducerConfig        - The configuration 'group.id' was supplied but isn't a known config.
23:06:51,757 WARN  producer.ProducerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:51,757 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:51,757 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:51,826 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910000 type:setData cxid:0xe zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics/test-topic-before-read Error:KeeperErrorCode = NoNode for /config/topics/test-topic-before-read
23:06:51,830 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910000 type:create cxid:0xf zxid:0x29 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
23:06:51,857 WARN  topic.KafkaTopic               - Since Southpaw handles its own offsets, the auto offset reset config is ignored. If there are no existing offsets, we will always start at the beginning.
23:06:51,857 INFO  consumer.ConsumerConfig        - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:34269]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

23:06:51,859 WARN  consumer.ConsumerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:51,859 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:51,859 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:51,876 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910001 type:create cxid:0x65 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic-before-read/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic-before-read/partitions/0
23:06:51,879 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa68910001 type:create cxid:0x67 zxid:0x2d txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic-before-read/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic-before-read/partitions
23:06:51,991 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:51,997 INFO  topic.KafkaTopic               - No offsets found for topic test, seeking to beginning.
23:06:51,997 WARN  topic.KafkaTopic               - It is recommended to set ACKS to 'all' otherwise data loss can occur
23:06:51,997 INFO  producer.ProducerConfig        - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:34269]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

23:06:52,001 WARN  producer.ProducerConfig        - The configuration 'enable.auto.commit' was supplied but isn't a known config.
23:06:52,001 WARN  producer.ProducerConfig        - The configuration 'group.id' was supplied but isn't a known config.
23:06:52,001 WARN  producer.ProducerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:52,001 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:52,001 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:52,089 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:06:52,090 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa347b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:52,129 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:52,172 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:52,181 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:06:52,214 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:06:52,315 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:06:52,331 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:06:52,516 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:06:52,532 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:06:52,616 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:52,918 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:06:52,936 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:06:53,049 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:39195. Will not attempt to authenticate using SASL (unknown error)
23:06:53,049 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa5c350000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:53,082 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:53,146 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:53,233 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:53,340 INFO  zkclient.ZkEventThread         - Terminate ZkClient event thread.
23:06:53,341 INFO  server.PrepRequestProcessor    - Processed session termination for sessionid: 0x173a1fa68910001
23:06:53,343 INFO  zookeeper.ZooKeeper            - Session: 0x173a1fa68910001 closed
23:06:53,344 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:33928 which had sessionid 0x173a1fa68910001
23:06:53,344 INFO  zookeeper.ClientCnxn           - EventThread shut down for session: 0x173a1fa68910001
23:06:53,345 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:33926 which had sessionid 0x173a1fa68910000
23:06:53,346 INFO  server.NIOServerCnxnFactory    - NIOServerCnxn factory exited run method
23:06:53,346 INFO  zookeeper.ClientCnxn           - Unable to read additional data from server sessionid 0x173a1fa68910000, likely server has closed socket, closing socket connection and attempting reconnect
23:06:53,348 INFO  server.ZooKeeperServer         - shutting down
23:06:53,348 INFO  server.SessionTrackerImpl      - Shutting down
23:06:53,348 INFO  server.PrepRequestProcessor    - Shutting down
23:06:53,348 INFO  server.SyncRequestProcessor    - Shutting down
23:06:53,348 INFO  server.PrepRequestProcessor    - PrepRequestProcessor exited loop!
23:06:53,348 INFO  server.SyncRequestProcessor    - SyncRequestProcessor exited!
23:06:53,349 INFO  server.FinalRequestProcessor   - shutdown of request processor complete
23:06:53,359 INFO  server.ZooKeeperServerMain     - Starting server
23:06:53,359 INFO  server.ZooKeeperServer         - tickTime set to 3000
23:06:53,359 INFO  server.ZooKeeperServer         - minSessionTimeout set to -1
23:06:53,359 INFO  server.ZooKeeperServer         - maxSessionTimeout set to -1
23:06:53,360 INFO  server.NIOServerCnxnFactory    - binding to port 0.0.0.0/0.0.0.0:45457
23:06:53,449 INFO  zkclient.ZkClient              - zookeeper state changed (Disconnected)
23:06:53,459 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:06:53,460 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa4d5b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:53,584 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:06:53,662 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:06:53,782 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:53,935 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:54,000 INFO  server.SessionTrackerImpl      - SessionTrackerImpl exited loop!
23:06:54,117 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:06:54,118 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa347b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:54,171 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:39195. Will not attempt to authenticate using SASL (unknown error)
23:06:54,171 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa5c350000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:54,288 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:54,352 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:54,385 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:45457 sessionTimeout=2147483647 watcher=org.I0Itec.zkclient.ZkClient@4247c273
23:06:54,385 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:06:54,392 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:06:54,393 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:45457. Will not attempt to authenticate using SASL (unknown error)
23:06:54,393 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:45457, initiating session
23:06:54,393 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:39914
23:06:54,394 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:39914
23:06:54,397 INFO  persistence.FileTxnLog         - Creating new log file: log.1
23:06:54,409 INFO  server.ZooKeeperServer         - Established session 0x173a1fa78330000 with negotiated timeout 60000 for client /127.0.0.1:39914
23:06:54,413 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:45457, sessionid = 0x173a1fa78330000, negotiated timeout = 60000
23:06:54,413 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:06:54,414 INFO  server.KafkaConfig             - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka/logs/log-35307
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 35307
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:45457
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

23:06:54,417 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:45457 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@13f2cf14
23:06:54,421 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:06:54,421 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:06:54,441 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:45457. Will not attempt to authenticate using SASL (unknown error)
23:06:54,441 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:45457, initiating session
23:06:54,441 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:39918
23:06:54,441 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:39918
23:06:54,444 INFO  server.ZooKeeperServer         - Established session 0x173a1fa78330001 with negotiated timeout 6000 for client /127.0.0.1:39918
23:06:54,444 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:45457, sessionid = 0x173a1fa78330001, negotiated timeout = 6000
23:06:54,444 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:06:54,463 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330001 type:create cxid:0x5 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
23:06:54,474 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330001 type:create cxid:0xb zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
23:06:54,491 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330001 type:create cxid:0x13 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
23:06:54,519 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330001 type:create cxid:0x1f zxid:0x14 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
23:06:54,529 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:46551. Will not attempt to authenticate using SASL (unknown error)
23:06:54,529 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa68910000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:54,617 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:06:54,697 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:06:54,697 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa4d5b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:54,718 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:06:54,739 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:54,853 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330001 type:setData cxid:0x2f zxid:0x19 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
23:06:54,885 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330001 type:delete cxid:0x46 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
23:06:54,905 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330001 type:create cxid:0x4b zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
23:06:54,905 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330001 type:create cxid:0x4c zxid:0x1d txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
23:06:54,933 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:54,933 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:54,935 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:54,977 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330000 type:setData cxid:0x5 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics/test-topic Error:KeeperErrorCode = NoNode for /config/topics/test-topic
23:06:54,997 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330000 type:create cxid:0x7 zxid:0x20 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
23:06:55,019 WARN  topic.KafkaTopic               - Since Southpaw handles its own offsets, the auto offset reset config is ignored. If there are no existing offsets, we will always start at the beginning.
23:06:55,021 INFO  consumer.ConsumerConfig        - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:35307]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

23:06:55,035 WARN  consumer.ConsumerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:55,036 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:55,036 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:55,049 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330001 type:create cxid:0x57 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions/0
23:06:55,057 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330001 type:create cxid:0x59 zxid:0x24 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions
23:06:55,201 INFO  topic.KafkaTopic               - No offsets found for topic test, seeking to beginning.
23:06:55,201 WARN  topic.KafkaTopic               - It is recommended to set ACKS to 'all' otherwise data loss can occur
23:06:55,202 INFO  producer.ProducerConfig        - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:35307]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

23:06:55,205 WARN  producer.ProducerConfig        - The configuration 'enable.auto.commit' was supplied but isn't a known config.
23:06:55,205 WARN  producer.ProducerConfig        - The configuration 'group.id' was supplied but isn't a known config.
23:06:55,205 WARN  producer.ProducerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:55,205 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:55,205 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:55,294 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330000 type:setData cxid:0xe zxid:0x28 txntype:-1 reqpath:n/a Error Path:/config/topics/test-topic-get-lag Error:KeeperErrorCode = NoNode for /config/topics/test-topic-get-lag
23:06:55,305 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330000 type:create cxid:0xf zxid:0x29 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
23:06:55,325 WARN  topic.KafkaTopic               - Since Southpaw handles its own offsets, the auto offset reset config is ignored. If there are no existing offsets, we will always start at the beginning.
23:06:55,328 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330001 type:create cxid:0x65 zxid:0x2c txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic-get-lag/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic-get-lag/partitions/0
23:06:55,330 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa78330001 type:create cxid:0x66 zxid:0x2d txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic-get-lag/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic-get-lag/partitions
23:06:55,334 INFO  consumer.ConsumerConfig        - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:35307]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

23:06:55,336 WARN  consumer.ConsumerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:55,336 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:55,336 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:55,441 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:55,444 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:06:55,445 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa347b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:55,460 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:55,468 INFO  topic.KafkaTopic               - No offsets found for topic test, seeking to beginning.
23:06:55,468 WARN  topic.KafkaTopic               - It is recommended to set ACKS to 'all' otherwise data loss can occur
23:06:55,468 INFO  producer.ProducerConfig        - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:35307]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

23:06:55,470 WARN  producer.ProducerConfig        - The configuration 'enable.auto.commit' was supplied but isn't a known config.
23:06:55,470 WARN  producer.ProducerConfig        - The configuration 'group.id' was supplied but isn't a known config.
23:06:55,470 WARN  producer.ProducerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:55,470 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:55,470 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:55,585 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:39195. Will not attempt to authenticate using SASL (unknown error)
23:06:55,585 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa5c350000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:55,612 WARN  clients.NetworkClient          - [Producer clientId=producer-7] Connection to node 0 could not be established. Broker may not be available.
23:06:55,623 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:55,623 WARN  clients.NetworkClient          - [Producer clientId=producer-8] Connection to node 0 could not be established. Broker may not be available.
23:06:55,704 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:06:55,713 WARN  clients.NetworkClient          - [Producer clientId=producer-7] Connection to node 0 could not be established. Broker may not be available.
23:06:55,724 WARN  clients.NetworkClient          - [Producer clientId=producer-8] Connection to node 0 could not be established. Broker may not be available.
23:06:55,844 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:55,913 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:06:55,914 WARN  clients.NetworkClient          - [Producer clientId=producer-7] Connection to node 0 could not be established. Broker may not be available.
23:06:55,975 WARN  clients.NetworkClient          - [Producer clientId=producer-8] Connection to node 0 could not be established. Broker may not be available.
23:06:56,257 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:06:56,257 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa4d5b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:56,287 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:46551. Will not attempt to authenticate using SASL (unknown error)
23:06:56,287 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa68910000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:56,315 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:56,315 WARN  clients.NetworkClient          - [Producer clientId=producer-7] Connection to node 0 could not be established. Broker may not be available.
23:06:56,427 WARN  clients.NetworkClient          - [Producer clientId=producer-8] Connection to node 0 could not be established. Broker may not be available.
23:06:56,496 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:56,583 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:56,807 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:06:56,817 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:06:57,018 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:57,072 WARN  clients.NetworkClient          - [Producer clientId=producer-7] Connection to node 0 could not be established. Broker may not be available.
23:06:57,168 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:06:57,169 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa347b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:57,322 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:57,385 WARN  clients.NetworkClient          - [Producer clientId=producer-8] Connection to node 0 could not be established. Broker may not be available.
23:06:57,586 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:57,617 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:39195. Will not attempt to authenticate using SASL (unknown error)
23:06:57,617 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa5c350000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:57,699 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:57,872 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:06:57,962 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:06:58,022 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:58,043 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:46551. Will not attempt to authenticate using SASL (unknown error)
23:06:58,044 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa68910000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:58,051 INFO  zkclient.ZkEventThread         - Terminate ZkClient event thread.
23:06:58,053 INFO  server.PrepRequestProcessor    - Processed session termination for sessionid: 0x173a1fa78330001
23:06:58,056 INFO  zookeeper.ZooKeeper            - Session: 0x173a1fa78330001 closed
23:06:58,057 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:39918 which had sessionid 0x173a1fa78330001
23:06:58,057 INFO  zookeeper.ClientCnxn           - EventThread shut down for session: 0x173a1fa78330001
23:06:58,062 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:39914 which had sessionid 0x173a1fa78330000
23:06:58,063 INFO  zookeeper.ClientCnxn           - Unable to read additional data from server sessionid 0x173a1fa78330000, likely server has closed socket, closing socket connection and attempting reconnect
23:06:58,066 INFO  server.NIOServerCnxnFactory    - NIOServerCnxn factory exited run method
23:06:58,067 INFO  server.ZooKeeperServer         - shutting down
23:06:58,067 INFO  server.SessionTrackerImpl      - Shutting down
23:06:58,067 INFO  server.PrepRequestProcessor    - Shutting down
23:06:58,067 INFO  server.SyncRequestProcessor    - Shutting down
23:06:58,067 INFO  server.PrepRequestProcessor    - PrepRequestProcessor exited loop!
23:06:58,067 INFO  server.SyncRequestProcessor    - SyncRequestProcessor exited!
23:06:58,067 INFO  server.FinalRequestProcessor   - shutdown of request processor complete
23:06:58,069 INFO  server.ZooKeeperServerMain     - Starting server
23:06:58,069 INFO  server.ZooKeeperServer         - tickTime set to 3000
23:06:58,069 INFO  server.ZooKeeperServer         - minSessionTimeout set to -1
23:06:58,069 INFO  server.ZooKeeperServer         - maxSessionTimeout set to -1
23:06:58,070 INFO  server.NIOServerCnxnFactory    - binding to port 0.0.0.0/0.0.0.0:41771
23:06:58,169 INFO  zkclient.ZkClient              - zookeeper state changed (Disconnected)
23:06:58,231 WARN  clients.NetworkClient          - [Producer clientId=producer-7] Connection to node 0 could not be established. Broker may not be available.
23:06:58,269 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:06:58,269 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa4d5b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:58,275 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:58,590 WARN  clients.NetworkClient          - [Producer clientId=producer-8] Connection to node 0 could not be established. Broker may not be available.
23:06:58,662 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:06:58,662 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa347b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:58,689 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:58,707 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:58,875 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:58,977 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:06:59,034 WARN  clients.NetworkClient          - [Producer clientId=producer-7] Connection to node 0 could not be established. Broker may not be available.
23:06:59,093 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:41771 sessionTimeout=2147483647 watcher=org.I0Itec.zkclient.ZkClient@2b8159d2
23:06:59,093 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:06:59,094 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:06:59,094 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:41771. Will not attempt to authenticate using SASL (unknown error)
23:06:59,094 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:41771, initiating session
23:06:59,095 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:56674
23:06:59,095 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:56674
23:06:59,099 INFO  persistence.FileTxnLog         - Creating new log file: log.1
23:06:59,113 INFO  server.ZooKeeperServer         - Established session 0x173a1fa8a990000 with negotiated timeout 60000 for client /127.0.0.1:56674
23:06:59,114 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:41771, sessionid = 0x173a1fa8a990000, negotiated timeout = 60000
23:06:59,114 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:06:59,115 INFO  server.KafkaConfig             - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka/logs/log-45645
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 45645
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:41771
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

23:06:59,116 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:06:59,117 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:41771 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@4cbc5f31
23:06:59,121 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:06:59,121 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:06:59,121 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:41771. Will not attempt to authenticate using SASL (unknown error)
23:06:59,121 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:41771, initiating session
23:06:59,121 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:56678
23:06:59,122 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:56678
23:06:59,124 INFO  server.ZooKeeperServer         - Established session 0x173a1fa8a990001 with negotiated timeout 6000 for client /127.0.0.1:56678
23:06:59,124 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:41771, sessionid = 0x173a1fa8a990001, negotiated timeout = 6000
23:06:59,124 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:06:59,133 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa8a990001 type:create cxid:0x5 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
23:06:59,144 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa8a990001 type:create cxid:0xb zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
23:06:59,158 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa8a990001 type:create cxid:0x13 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
23:06:59,178 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa8a990001 type:create cxid:0x1f zxid:0x14 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
23:06:59,178 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:06:59,187 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:46551. Will not attempt to authenticate using SASL (unknown error)
23:06:59,187 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa68910000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:59,446 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:39195. Will not attempt to authenticate using SASL (unknown error)
23:06:59,456 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa5c350000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:59,479 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa8a990001 type:setData cxid:0x2a zxid:0x18 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
23:06:59,488 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa8a990001 type:delete cxid:0x4a zxid:0x1b txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
23:06:59,489 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa8a990001 type:create cxid:0x4b zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
23:06:59,489 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa8a990001 type:create cxid:0x4c zxid:0x1d txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
23:06:59,501 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:59,501 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:59,503 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa8a990000 type:setData cxid:0x5 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics/test-topic Error:KeeperErrorCode = NoNode for /config/topics/test-topic
23:06:59,506 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa8a990000 type:create cxid:0x7 zxid:0x20 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
23:06:59,515 WARN  topic.KafkaTopic               - Since Southpaw handles its own offsets, the auto offset reset config is ignored. If there are no existing offsets, we will always start at the beginning.
23:06:59,515 INFO  consumer.ConsumerConfig        - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:45645]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

23:06:59,516 WARN  consumer.ConsumerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:59,516 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:59,516 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:59,519 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa8a990001 type:create cxid:0x57 zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions/0
23:06:59,523 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa8a990001 type:create cxid:0x5a zxid:0x24 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions
23:06:59,544 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:45457. Will not attempt to authenticate using SASL (unknown error)
23:06:59,544 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa78330000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:59,635 INFO  topic.KafkaTopic               - No offsets found for topic test, seeking to beginning.
23:06:59,635 WARN  topic.KafkaTopic               - It is recommended to set ACKS to 'all' otherwise data loss can occur
23:06:59,635 INFO  producer.ProducerConfig        - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:45645]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

23:06:59,636 WARN  producer.ProducerConfig        - The configuration 'enable.auto.commit' was supplied but isn't a known config.
23:06:59,636 WARN  producer.ProducerConfig        - The configuration 'group.id' was supplied but isn't a known config.
23:06:59,637 WARN  producer.ProducerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:59,637 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:59,637 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:59,651 WARN  topic.KafkaTopic               - Since Southpaw handles its own offsets, the auto offset reset config is ignored. If there are no existing offsets, we will always start at the beginning.
23:06:59,651 INFO  consumer.ConsumerConfig        - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:45645]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

23:06:59,652 WARN  consumer.ConsumerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:59,652 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:59,652 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:59,655 INFO  topic.KafkaTopic               - No offsets found for topic test, seeking to beginning.
23:06:59,656 WARN  topic.KafkaTopic               - It is recommended to set ACKS to 'all' otherwise data loss can occur
23:06:59,656 INFO  producer.ProducerConfig        - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:45645]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

23:06:59,657 WARN  producer.ProducerConfig        - The configuration 'enable.auto.commit' was supplied but isn't a known config.
23:06:59,657 WARN  producer.ProducerConfig        - The configuration 'group.id' was supplied but isn't a known config.
23:06:59,657 WARN  producer.ProducerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:06:59,657 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:06:59,657 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:06:59,697 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:06:59,740 WARN  clients.NetworkClient          - [Producer clientId=producer-9] Connection to node 0 could not be established. Broker may not be available.
23:06:59,748 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:06:59,749 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:06:59,749 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa4d5b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:06:59,761 WARN  clients.NetworkClient          - [Producer clientId=producer-10] Connection to node 0 could not be established. Broker may not be available.
23:06:59,798 WARN  clients.NetworkClient          - [Producer clientId=producer-8] Connection to node 0 could not be established. Broker may not be available.
23:06:59,841 WARN  clients.NetworkClient          - [Producer clientId=producer-9] Connection to node 0 could not be established. Broker may not be available.
23:06:59,865 WARN  clients.NetworkClient          - [Producer clientId=producer-10] Connection to node 0 could not be established. Broker may not be available.
23:06:59,899 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:06:59,901 WARN  clients.NetworkClient          - [Producer clientId=producer-7] Connection to node 0 could not be established. Broker may not be available.
23:06:59,950 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:07:00,001 INFO  server.SessionTrackerImpl      - SessionTrackerImpl exited loop!
23:07:00,049 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:07:00,066 WARN  clients.NetworkClient          - [Producer clientId=producer-10] Connection to node 0 could not be established. Broker may not be available.
23:07:00,073 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:07:00,073 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa347b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:00,093 WARN  clients.NetworkClient          - [Producer clientId=producer-9] Connection to node 0 could not be established. Broker may not be available.
23:07:00,100 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:07:00,445 WARN  clients.NetworkClient          - [Producer clientId=producer-9] Connection to node 0 could not be established. Broker may not be available.
23:07:00,518 WARN  clients.NetworkClient          - [Producer clientId=producer-10] Connection to node 0 could not be established. Broker may not be available.
23:07:00,700 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:07:00,749 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:46551. Will not attempt to authenticate using SASL (unknown error)
23:07:00,749 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa68910000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:00,753 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:07:00,803 WARN  clients.NetworkClient          - [Producer clientId=producer-7] Connection to node 0 could not be established. Broker may not be available.
23:07:00,851 WARN  clients.NetworkClient          - [Producer clientId=producer-8] Connection to node 0 could not be established. Broker may not be available.
23:07:00,904 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:07:00,910 INFO  zkclient.ZkEventThread         - Terminate ZkClient event thread.
23:07:00,910 INFO  server.PrepRequestProcessor    - Processed session termination for sessionid: 0x173a1fa8a990001
23:07:00,916 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:56678 which had sessionid 0x173a1fa8a990001
23:07:00,916 INFO  zookeeper.ZooKeeper            - Session: 0x173a1fa8a990001 closed
23:07:00,916 INFO  zookeeper.ClientCnxn           - EventThread shut down for session: 0x173a1fa8a990001
23:07:00,920 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:56674 which had sessionid 0x173a1fa8a990000
23:07:00,920 INFO  server.NIOServerCnxnFactory    - NIOServerCnxn factory exited run method
23:07:00,920 INFO  zookeeper.ClientCnxn           - Unable to read additional data from server sessionid 0x173a1fa8a990000, likely server has closed socket, closing socket connection and attempting reconnect
23:07:00,929 INFO  server.ZooKeeperServer         - shutting down
23:07:00,929 INFO  server.SessionTrackerImpl      - Shutting down
23:07:00,929 INFO  server.PrepRequestProcessor    - Shutting down
23:07:00,929 INFO  server.SyncRequestProcessor    - Shutting down
23:07:00,929 INFO  server.PrepRequestProcessor    - PrepRequestProcessor exited loop!
23:07:00,929 INFO  server.SyncRequestProcessor    - SyncRequestProcessor exited!
23:07:00,929 INFO  server.FinalRequestProcessor   - shutdown of request processor complete
23:07:00,945 INFO  server.ZooKeeperServerMain     - Starting server
23:07:00,945 INFO  server.ZooKeeperServer         - tickTime set to 3000
23:07:00,945 INFO  server.ZooKeeperServer         - minSessionTimeout set to -1
23:07:00,945 INFO  server.ZooKeeperServer         - maxSessionTimeout set to -1
23:07:00,945 INFO  server.NIOServerCnxnFactory    - binding to port 0.0.0.0/0.0.0.0:33975
23:07:01,004 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:07:01,005 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:07:01,009 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:07:01,023 INFO  zkclient.ZkClient              - zookeeper state changed (Disconnected)
23:07:01,132 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:39195. Will not attempt to authenticate using SASL (unknown error)
23:07:01,133 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa5c350000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:01,238 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:07:01,238 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa347b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:01,247 WARN  clients.NetworkClient          - [Producer clientId=producer-9] Connection to node 0 could not be established. Broker may not be available.
23:07:01,271 WARN  clients.NetworkClient          - [Producer clientId=producer-10] Connection to node 0 could not be established. Broker may not be available.
23:07:01,406 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:45457. Will not attempt to authenticate using SASL (unknown error)
23:07:01,406 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa78330000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:01,605 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:07:01,760 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:07:01,810 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:07:01,810 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa4d5b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:01,855 WARN  clients.NetworkClient          - [Producer clientId=producer-8] Connection to node 0 could not be established. Broker may not be available.
23:07:01,860 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:07:01,957 WARN  clients.NetworkClient          - [Producer clientId=producer-7] Connection to node 0 could not be established. Broker may not be available.
23:07:01,961 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:33975 sessionTimeout=2147483647 watcher=org.I0Itec.zkclient.ZkClient@31731168
23:07:01,961 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:07:01,977 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:07:01,977 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:33975. Will not attempt to authenticate using SASL (unknown error)
23:07:01,977 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:33975, initiating session
23:07:01,981 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:38952
23:07:01,981 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:38952
23:07:01,982 INFO  persistence.FileTxnLog         - Creating new log file: log.1
23:07:02,006 INFO  server.ZooKeeperServer         - Established session 0x173a1fa95d50000 with negotiated timeout 60000 for client /127.0.0.1:38952
23:07:02,008 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:07:02,009 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:33975, sessionid = 0x173a1fa95d50000, negotiated timeout = 60000
23:07:02,009 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:07:02,010 INFO  server.KafkaConfig             - KafkaConfig values: 
	advertised.host.name = null
	advertised.listeners = null
	advertised.port = null
	alter.config.policy.class.name = null
	authorizer.class.name = 
	auto.create.topics.enable = true
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.id = 0
	broker.id.generation.enable = true
	broker.rack = null
	compression.type = producer
	connections.max.idle.ms = 600000
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 1
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	fetch.purgatory.purge.interval.requests = 1000
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 300000
	group.min.session.timeout.ms = 6000
	host.name = 
	inter.broker.listener.name = null
	inter.broker.protocol.version = 1.0-IV0
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	listener.security.protocol.map = PLAINTEXT:PLAINTEXT,SSL:SSL,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
	listeners = null
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.dir = /tmp/kafka/logs/log-41783
	log.dirs = null
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.format.version = 1.0-IV0
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	message.max.bytes = 1000012
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 1
	num.io.threads = 8
	num.network.threads = 3
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 5000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 1440
	offsets.topic.compression.codec = 0
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 1
	offsets.topic.segment.bytes = 104857600
	port = 41783
	principal.builder.class = null
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.consumer.default = 9223372036854775807
	quota.producer.default = 9223372036854775807
	quota.window.num = 11
	quota.window.size.seconds = 1
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 10000
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.enabled.mechanisms = [GSSAPI]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism.inter.broker.protocol = GSSAPI
	security.inter.broker.protocol = PLAINTEXT
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = null
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 60000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.connect = 127.0.0.1:33975
	zookeeper.connection.timeout.ms = null
	zookeeper.session.timeout.ms = 6000
	zookeeper.set.acl = false
	zookeeper.sync.time.ms = 2000

23:07:02,010 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:07:02,025 INFO  zookeeper.ZooKeeper            - Initiating client connection, connectString=127.0.0.1:33975 sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@1cec92b8
23:07:02,028 INFO  zkclient.ZkEventThread         - Starting ZkClient event thread.
23:07:02,049 INFO  zkclient.ZkClient              - Waiting for keeper state SyncConnected
23:07:02,049 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:33975. Will not attempt to authenticate using SASL (unknown error)
23:07:02,049 INFO  zookeeper.ClientCnxn           - Socket connection established to 127.0.0.1/127.0.0.1:33975, initiating session
23:07:02,049 INFO  server.NIOServerCnxnFactory    - Accepted socket connection from /127.0.0.1:38960
23:07:02,049 INFO  server.ZooKeeperServer         - Client attempting to establish new session at /127.0.0.1:38960
23:07:02,056 INFO  server.ZooKeeperServer         - Established session 0x173a1fa95d50001 with negotiated timeout 6000 for client /127.0.0.1:38960
23:07:02,057 INFO  zookeeper.ClientCnxn           - Session establishment complete on server 127.0.0.1/127.0.0.1:33975, sessionid = 0x173a1fa95d50001, negotiated timeout = 6000
23:07:02,057 INFO  zkclient.ZkClient              - zookeeper state changed (SyncConnected)
23:07:02,065 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:07:02,098 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa95d50001 type:create cxid:0x5 zxid:0x4 txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NoNode for /brokers
23:07:02,110 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa95d50001 type:create cxid:0xb zxid:0x8 txntype:-1 reqpath:n/a Error Path:/config Error:KeeperErrorCode = NoNode for /config
23:07:02,125 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa95d50001 type:create cxid:0x13 zxid:0xd txntype:-1 reqpath:n/a Error Path:/admin Error:KeeperErrorCode = NoNode for /admin
23:07:02,135 WARN  clients.NetworkClient          - [Producer clientId=producer-10] Connection to node 0 could not be established. Broker may not be available.
23:07:02,146 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa95d50001 type:create cxid:0x1f zxid:0x14 txntype:-1 reqpath:n/a Error Path:/cluster Error:KeeperErrorCode = NoNode for /cluster
23:07:02,321 WARN  clients.NetworkClient          - [Producer clientId=producer-9] Connection to node 0 could not be established. Broker may not be available.
23:07:02,418 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa95d50001 type:setData cxid:0x2c zxid:0x19 txntype:-1 reqpath:n/a Error Path:/controller_epoch Error:KeeperErrorCode = NoNode for /controller_epoch
23:07:02,469 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:41771. Will not attempt to authenticate using SASL (unknown error)
23:07:02,470 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa8a990000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:02,472 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa95d50001 type:delete cxid:0x41 zxid:0x1b txntype:-1 reqpath:n/a Error Path:/admin/preferred_replica_election Error:KeeperErrorCode = NoNode for /admin/preferred_replica_election
23:07:02,569 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa95d50001 type:create cxid:0x4b zxid:0x1c txntype:-1 reqpath:n/a Error Path:/brokers Error:KeeperErrorCode = NodeExists for /brokers
23:07:02,569 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa95d50001 type:create cxid:0x4c zxid:0x1d txntype:-1 reqpath:n/a Error Path:/brokers/ids Error:KeeperErrorCode = NodeExists for /brokers/ids
23:07:02,581 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:46551. Will not attempt to authenticate using SASL (unknown error)
23:07:02,581 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa68910000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:02,592 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:07:02,593 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:07:02,632 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:07:02,632 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa347b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:02,637 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa95d50000 type:setData cxid:0x5 zxid:0x1f txntype:-1 reqpath:n/a Error Path:/config/topics/test-topic Error:KeeperErrorCode = NoNode for /config/topics/test-topic
23:07:02,653 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa95d50000 type:create cxid:0x7 zxid:0x20 txntype:-1 reqpath:n/a Error Path:/config/topics Error:KeeperErrorCode = NodeExists for /config/topics
23:07:02,667 WARN  topic.KafkaTopic               - Since Southpaw handles its own offsets, the auto offset reset config is ignored. If there are no existing offsets, we will always start at the beginning.
23:07:02,668 INFO  consumer.ConsumerConfig        - ConsumerConfig values: 
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [localhost:41783]
	check.crcs = true
	client.id = 
	connections.max.idle.ms = 540000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-group
	heartbeat.interval.ms = 3000
	interceptor.classes = null
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 305000
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

23:07:02,669 WARN  consumer.ConsumerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:07:02,669 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:07:02,669 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:07:02,707 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa95d50001 type:create cxid:0x5a zxid:0x23 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions/0 Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions/0
23:07:02,711 INFO  server.PrepRequestProcessor    - Got user-level KeeperException when processing sessionid:0x173a1fa95d50001 type:create cxid:0x5b zxid:0x24 txntype:-1 reqpath:n/a Error Path:/brokers/topics/test-topic/partitions Error:KeeperErrorCode = NoNode for /brokers/topics/test-topic/partitions
23:07:02,757 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:07:02,773 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:45457. Will not attempt to authenticate using SASL (unknown error)
23:07:02,773 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa78330000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:02,781 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:07:02,825 INFO  topic.KafkaTopic               - No offsets found for topic test, seeking to beginning.
23:07:02,825 WARN  topic.KafkaTopic               - It is recommended to set ACKS to 'all' otherwise data loss can occur
23:07:02,826 INFO  producer.ProducerConfig        - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:41783]
	buffer.memory = 33554432
	client.id = 
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = null
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

23:07:02,837 WARN  producer.ProducerConfig        - The configuration 'enable.auto.commit' was supplied but isn't a known config.
23:07:02,837 WARN  producer.ProducerConfig        - The configuration 'group.id' was supplied but isn't a known config.
23:07:02,837 WARN  producer.ProducerConfig        - The configuration 'topic.name' was supplied but isn't a known config.
23:07:02,837 INFO  utils.AppInfoParser            - Kafka version : 1.0.0
23:07:02,837 INFO  utils.AppInfoParser            - Kafka commitId : aaa7af6d4a11b29d
23:07:02,889 INFO  topic.KafkaTopic               - Resetting offsets for topic test, seeking to beginning.
23:07:02,945 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:07:02,965 WARN  clients.NetworkClient          - [Producer clientId=producer-10] Connection to node 0 could not be established. Broker may not be available.
23:07:02,980 WARN  clients.NetworkClient          - [Producer clientId=producer-11] Connection to node 0 could not be established. Broker may not be available.
23:07:02,990 WARN  clients.NetworkClient          - [Producer clientId=producer-7] Connection to node 0 could not be established. Broker may not be available.
23:07:03,001 INFO  server.SessionTrackerImpl      - SessionTrackerImpl exited loop!
23:07:03,049 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:07:03,050 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:39195. Will not attempt to authenticate using SASL (unknown error)
23:07:03,051 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa5c350000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:03,072 WARN  clients.NetworkClient          - [Producer clientId=producer-8] Connection to node 0 could not be established. Broker may not be available.
23:07:03,076 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:07:03,080 WARN  clients.NetworkClient          - [Producer clientId=producer-11] Connection to node 0 could not be established. Broker may not be available.
23:07:03,234 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:07:03,331 WARN  clients.NetworkClient          - [Producer clientId=producer-11] Connection to node 0 could not be established. Broker may not be available.
23:07:03,467 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:07:03,467 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa4d5b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:03,502 WARN  clients.NetworkClient          - [Producer clientId=producer-9] Connection to node 0 could not be established. Broker may not be available.
23:07:03,573 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:07:03,683 WARN  clients.NetworkClient          - [Producer clientId=producer-11] Connection to node 0 could not be established. Broker may not be available.
23:07:03,836 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:07:03,842 WARN  clients.NetworkClient          - [Producer clientId=producer-7] Connection to node 0 could not be established. Broker may not be available.
23:07:03,924 WARN  clients.NetworkClient          - [Producer clientId=producer-8] Connection to node 0 could not be established. Broker may not be available.
23:07:03,981 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:41771. Will not attempt to authenticate using SASL (unknown error)
23:07:03,981 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa8a990000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:04,080 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:07:04,099 WARN  clients.NetworkClient          - [Producer clientId=producer-1] Connection to node 0 could not be established. Broker may not be available.
23:07:04,124 WARN  clients.NetworkClient          - [Producer clientId=producer-10] Connection to node 0 could not be established. Broker may not be available.
23:07:04,183 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:46551. Will not attempt to authenticate using SASL (unknown error)
23:07:04,183 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa68910000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:04,202 WARN  clients.NetworkClient          - [Producer clientId=producer-6] Connection to node 0 could not be established. Broker may not be available.
23:07:04,240 WARN  clients.NetworkClient          - [Producer clientId=producer-3] Connection to node 0 could not be established. Broker may not be available.
23:07:04,267 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:35697. Will not attempt to authenticate using SASL (unknown error)
23:07:04,267 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa347b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:04,284 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:45457. Will not attempt to authenticate using SASL (unknown error)
23:07:04,284 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa78330000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:04,462 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:39195. Will not attempt to authenticate using SASL (unknown error)
23:07:04,463 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa5c350000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:04,489 WARN  clients.NetworkClient          - [Producer clientId=producer-11] Connection to node 0 could not be established. Broker may not be available.
23:07:04,510 WARN  clients.NetworkClient          - [Producer clientId=producer-9] Connection to node 0 could not be established. Broker may not be available.
23:07:04,626 WARN  clients.NetworkClient          - [Producer clientId=producer-4] Connection to node 0 could not be established. Broker may not be available.
23:07:04,645 WARN  clients.NetworkClient          - [Producer clientId=producer-7] Connection to node 0 could not be established. Broker may not be available.
23:07:04,689 WARN  clients.NetworkClient          - [Producer clientId=producer-2] Connection to node 0 could not be established. Broker may not be available.
23:07:04,776 WARN  clients.NetworkClient          - [Producer clientId=producer-8] Connection to node 0 could not be established. Broker may not be available.
23:07:04,822 INFO  zookeeper.ClientCnxn           - Opening socket connection to server 127.0.0.1/127.0.0.1:36045. Will not attempt to authenticate using SASL (unknown error)
23:07:04,822 WARN  zookeeper.ClientCnxn           - Session 0x173a1fa4d5b0000 for server null, unexpected error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) ~[?:1.8.0_262]
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:714) ~[?:1.8.0_262]
	at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361) ~[zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1141) [zookeeper-3.4.10.jar:3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f]
23:07:05,029 INFO  zkclient.ZkEventThread         - Terminate ZkClient event thread.
23:07:05,031 INFO  server.PrepRequestProcessor    - Processed session termination for sessionid: 0x173a1fa95d50001
23:07:05,033 WARN  clients.NetworkClient          - [Producer clientId=producer-5] Connection to node 0 could not be established. Broker may not be available.
23:07:05,035 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:38960 which had sessionid 0x173a1fa95d50001
23:07:05,035 INFO  zookeeper.ZooKeeper            - Session: 0x173a1fa95d50001 closed
23:07:05,035 INFO  zookeeper.ClientCnxn           - EventThread shut down for session: 0x173a1fa95d50001
23:07:05,043 INFO  server.NIOServerCnxn           - Closed socket connection for client /127.0.0.1:38952 which had sessionid 0x173a1fa95d50000
23:07:05,044 INFO  zookeeper.ClientCnxn           - Unable to read additional data from server sessionid 0x173a1fa95d50000, likely server has closed socket, closing socket connection and attempting reconnect
23:07:05,045 INFO  server.NIOServerCnxnFactory    - NIOServerCnxn factory exited run method
23:07:05,049 INFO  server.ZooKeeperServer         - shutting down
23:07:05,049 INFO  server.SessionTrackerImpl      - Shutting down
23:07:05,049 INFO  server.PrepRequestProcessor    - Shutting down
23:07:05,050 INFO  server.SyncRequestProcessor    - Shutting down
23:07:05,050 INFO  server.PrepRequestProcessor    - PrepRequestProcessor exited loop!
23:07:05,053 INFO  server.SyncRequestProcessor    - SyncRequestProcessor exited!
23:07:05,062 INFO  server.FinalRequestProcessor   - shutdown of request processor complete
Tests run: 7, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 29.352 sec - in com.jwplayer.southpaw.topic.KafkaTopicTest
Running com.jwplayer.southpaw.topic.InMemoryTopicTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in com.jwplayer.southpaw.topic.InMemoryTopicTest
Running com.jwplayer.southpaw.filter.BaseFilterTest
Tests run: 6, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0 sec - in com.jwplayer.southpaw.filter.BaseFilterTest

Results :

Tests run: 174, Failures: 0, Errors: 0, Skipped: 0

[INFO] 
[INFO] --- jacoco-maven-plugin:0.8.5:report (report) @ southpaw ---
[INFO] Loading execution data file /tmp/tmph9yrc09t/southpaw/target/jacoco.exec
[INFO] Analyzed bundle 'southpaw' with 93 classes
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  03:03 min
[INFO] Finished at: 2020-07-30T23:07:07Z
[INFO] ------------------------------------------------------------------------
